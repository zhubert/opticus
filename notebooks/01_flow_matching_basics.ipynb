{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Phase 1: Flow Matching Fundamentals\n",
    "\n",
    "In this notebook, we build a generative model for MNIST digits using **flow matching** - a clean and intuitive approach to learning generative models.\n",
    "\n",
    "## The Goal of Generative Modeling\n",
    "\n",
    "We want to learn to generate new data samples that look like our training data. Mathematically:\n",
    "\n",
    "- We have training data $x \\sim p_{\\text{data}}(x)$ (e.g., real MNIST digits)\n",
    "- We want to sample from $p_{\\text{data}}$ even though we don't know its explicit form\n",
    "\n",
    "The strategy: learn to **transform** samples from a simple distribution (Gaussian noise) into samples from the complex data distribution.\n",
    "\n",
    "## Why Flow Matching?\n",
    "\n",
    "There are several approaches to generative modeling:\n",
    "\n",
    "| Approach | Key Idea | Complexity |\n",
    "|----------|----------|------------|\n",
    "| **GANs** | Adversarial game between generator and discriminator | Training instability |\n",
    "| **VAEs** | Encode to latent space, decode back | Blurry outputs |\n",
    "| **DDPM** | Add noise gradually, learn to reverse it | Stochastic, many steps |\n",
    "| **Flow Matching** | Learn straight paths from noise to data | Simple, deterministic |\n",
    "\n",
    "Flow matching has gained popularity because:\n",
    "1. **Simpler mathematics** than DDPM (no stochastic differential equations)\n",
    "2. **Faster sampling** (straight paths are more efficient)\n",
    "3. **Same training objective** works for any architecture\n",
    "4. **State-of-the-art results** (used in Stable Diffusion 3, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-overview",
   "metadata": {},
   "source": [
    "## The Mathematical Framework\n",
    "\n",
    "### The Core Idea: Probability Paths\n",
    "\n",
    "Imagine we have two distributions:\n",
    "- $p_0$: The data distribution (complex, unknown) - what we want to sample from\n",
    "- $p_1$: A simple distribution (standard Gaussian) - easy to sample from\n",
    "\n",
    "Flow matching constructs a **continuous path of distributions** $p_t$ that smoothly transitions from $p_0$ to $p_1$ as $t$ goes from 0 to 1:\n",
    "\n",
    "$$p_0 \\xrightarrow{t=0 \\to 1} p_1$$\n",
    "\n",
    "If we can describe how individual samples move along this path, we can:\n",
    "1. **Train**: Learn the \"velocity\" of samples at each point\n",
    "2. **Generate**: Start from $p_1$ (noise) and follow velocities backward to $p_0$ (data)\n",
    "\n",
    "### The Probability Path via Interpolation\n",
    "\n",
    "The simplest way to construct this path is **linear interpolation**. For a data point $x_0$ and a noise sample $x_1$:\n",
    "\n",
    "$$x_t = (1-t) \\cdot x_0 + t \\cdot x_1$$\n",
    "\n",
    "This traces a straight line in pixel space from $x_0$ (at $t=0$) to $x_1$ (at $t=1$).\n",
    "\n",
    "**Intuition**: At $t=0$, we have the clean image. At $t=1$, we have pure noise. At $t=0.5$, we have a 50-50 blend.\n",
    "\n",
    "### The Velocity Field\n",
    "\n",
    "The **velocity** tells us how $x_t$ changes as $t$ increases. Taking the derivative:\n",
    "\n",
    "$$v = \\frac{dx_t}{dt} = \\frac{d}{dt}\\left[(1-t) \\cdot x_0 + t \\cdot x_1\\right] = -x_0 + x_1 = x_1 - x_0$$\n",
    "\n",
    "**Key insight**: The velocity is **constant** along the path! It doesn't depend on $t$. This means:\n",
    "- Each point travels in a straight line\n",
    "- The velocity is simply the direction from data to noise\n",
    "\n",
    "### The Neural Network's Job\n",
    "\n",
    "We train a neural network $v_\\theta(x_t, t)$ to predict this velocity given:\n",
    "- $x_t$: The current (noised) sample\n",
    "- $t$: The current timestep\n",
    "\n",
    "The training loss is simple **mean squared error**:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, x_1, t}\\left[\\|v_\\theta(x_t, t) - (x_1 - x_0)\\|^2\\right]$$\n",
    "\n",
    "### Generating Samples (The ODE)\n",
    "\n",
    "Once trained, we generate samples by solving an **ordinary differential equation** (ODE):\n",
    "\n",
    "$$\\frac{dx_t}{dt} = v_\\theta(x_t, t)$$\n",
    "\n",
    "We start at $t=1$ with noise $x_1 \\sim \\mathcal{N}(0, I)$ and integrate backward to $t=0$:\n",
    "\n",
    "$$x_0 = x_1 - \\int_1^0 v_\\theta(x_t, t) \\, dt$$\n",
    "\n",
    "In practice, we use **Euler integration**:\n",
    "\n",
    "$$x_{t-\\Delta t} = x_t - \\Delta t \\cdot v_\\theta(x_t, t)$$\n",
    "\n",
    "Repeat for many small steps from $t=1$ to $t=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-summary",
   "metadata": {},
   "source": [
    "### Summary of Key Equations\n",
    "\n",
    "| Concept | Equation | Description |\n",
    "|---------|----------|-------------|\n",
    "| **Interpolation** | $x_t = (1-t)x_0 + tx_1$ | Straight path from data to noise |\n",
    "| **Velocity** | $v = x_1 - x_0$ | Constant direction along path |\n",
    "| **Training Loss** | $\\mathcal{L} = \\|v_\\theta(x_t, t) - v\\|^2$ | MSE between predicted and true velocity |\n",
    "| **Sampling ODE** | $\\frac{dx}{dt} = v_\\theta(x, t)$ | Follow velocity field backward |\n",
    "| **Euler Step** | $x_{t-\\Delta t} = x_t - \\Delta t \\cdot v_\\theta$ | Discrete approximation |\n",
    "\n",
    "Now let's implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Auto-reload modules during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## 1. The Data Distribution: MNIST\n",
    "\n",
    "We use MNIST (28×28 grayscale handwritten digits) as our target distribution $p_{\\text{data}}$.\n",
    "\n",
    "MNIST is ideal for learning because:\n",
    "- Small images = fast training\n",
    "- Simple structure = easy to verify results visually\n",
    "- Yet complex enough to be interesting (10 digit classes, varying styles)\n",
    "\n",
    "**Preprocessing**: We normalize pixel values to $[-1, 1]$ (instead of $[0, 1]$) so they're centered around zero, matching the Gaussian noise distribution we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: convert to tensor and normalize to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Maps [0,1] to [-1,1]\n",
    "])\n",
    "\n",
    "# Download and load MNIST\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for MPS compatibility\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset):,} images\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"Pixel range: [{train_dataset[0][0].min():.1f}, {train_dataset[0][0].max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "def show_images(images, nrow=8, title=\"\"):\n",
    "    \"\"\"Display a grid of images.\"\"\"\n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    images = (images + 1) / 2\n",
    "    images = images.clamp(0, 1)\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow, padding=2)\n",
    "    plt.figure(figsize=(12, 12 * grid.shape[1] / grid.shape[2]))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch and visualize\n",
    "sample_batch, _ = next(iter(train_loader))\n",
    "show_images(sample_batch[:32], title=\"Training samples from $p_{data}$ (MNIST digits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-intro",
   "metadata": {},
   "source": [
    "## 2. The Forward Process: Data → Noise\n",
    "\n",
    "Now let's implement and visualize the interpolation path from data to noise.\n",
    "\n",
    "### Mathematical Recap\n",
    "\n",
    "For a data point $x_0$ and noise sample $x_1 \\sim \\mathcal{N}(0, I)$:\n",
    "\n",
    "$$x_t = (1-t) \\cdot x_0 + t \\cdot x_1$$\n",
    "\n",
    "At different timesteps:\n",
    "- $t = 0$: $x_0$ (pure data)\n",
    "- $t = 0.5$: $0.5 \\cdot x_0 + 0.5 \\cdot x_1$ (half data, half noise)\n",
    "- $t = 1$: $x_1$ (pure noise)\n",
    "\n",
    "### Geometric Interpretation\n",
    "\n",
    "Think of the 784-dimensional space (28×28 pixels) where each image is a point:\n",
    "- $x_0$ is a point in the \"data manifold\" (where real digits live)\n",
    "- $x_1$ is a random point (Gaussian noise, spread throughout space)\n",
    "- $x_t$ traces a straight line between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.flow import FlowMatching\n",
    "\n",
    "flow = FlowMatching()\n",
    "\n",
    "# Take one image and show its path to noise\n",
    "x_0 = sample_batch[0:1]  # Shape: (1, 1, 28, 28)\n",
    "x_1 = torch.randn_like(x_0)  # Sample noise ~ N(0, I)\n",
    "\n",
    "# Show interpolation at different timesteps\n",
    "timesteps = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "interpolated = []\n",
    "\n",
    "for t in timesteps:\n",
    "    t_tensor = torch.tensor([t])\n",
    "    x_t, velocity = flow.forward_process(x_0, x_1, t_tensor)\n",
    "    interpolated.append(x_t)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(15, 3))\n",
    "for i, (ax, t) in enumerate(zip(axes, timesteps)):\n",
    "    img = (interpolated[i][0, 0] + 1) / 2  # Denormalize\n",
    "    ax.imshow(img.numpy(), cmap='gray')\n",
    "    ax.set_title(f'$t = {t}$', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add equation annotation\n",
    "    if t == 0.0:\n",
    "        ax.set_xlabel('$x_0$ (data)', fontsize=10)\n",
    "    elif t == 1.0:\n",
    "        ax.set_xlabel('$x_1$ (noise)', fontsize=10)\n",
    "    elif t == 0.5:\n",
    "        ax.set_xlabel('$0.5 x_0 + 0.5 x_1$', fontsize=10)\n",
    "\n",
    "plt.suptitle('Forward Process: $x_t = (1-t) x_0 + t x_1$', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserve how the digit structure gradually dissolves into noise.\")\n",
    "print(\"The model will learn to reverse this process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velocity-intro",
   "metadata": {},
   "source": [
    "## 3. The Velocity Field\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "The velocity is the time derivative of the interpolation:\n",
    "\n",
    "$$v = \\frac{dx_t}{dt} = \\frac{d}{dt}\\left[(1-t) x_0 + t x_1\\right]$$\n",
    "\n",
    "Using the product rule:\n",
    "$$v = -x_0 + x_1 = x_1 - x_0$$\n",
    "\n",
    "**Key property**: The velocity is **independent of $t$**! This means:\n",
    "1. Each sample travels at constant velocity\n",
    "2. Paths are perfectly straight lines\n",
    "3. The velocity equals the displacement from data to noise\n",
    "\n",
    "### What Does the Velocity Look Like?\n",
    "\n",
    "Since $v = x_1 - x_0$:\n",
    "- Where $x_1 > x_0$ (noise brighter than data): velocity is positive (pixel gets brighter)\n",
    "- Where $x_1 < x_0$ (noise darker than data): velocity is negative (pixel gets darker)\n",
    "- The velocity essentially \"encodes\" how to transform this specific data point into this specific noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-velocity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize the velocity field for a single example\n",
    "t_tensor = torch.tensor([0.5])  # Timestep doesn't matter for velocity!\n",
    "x_t, velocity = flow.forward_process(x_0, x_1, t_tensor)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Data point x_0\n",
    "axes[0].imshow((x_0[0, 0] + 1) / 2, cmap='gray')\n",
    "axes[0].set_title('$x_0$ (data)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Noise point x_1\n",
    "axes[1].imshow((x_1[0, 0] + 1) / 2, cmap='gray')\n",
    "axes[1].set_title('$x_1$ (noise)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Velocity v = x_1 - x_0\n",
    "v_display = velocity[0, 0].numpy()\n",
    "im = axes[2].imshow(v_display, cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[2].set_title('$v = x_1 - x_0$ (velocity)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im, ax=axes[2], fraction=0.046)\n",
    "\n",
    "# Interpolated sample at t=0.5\n",
    "axes[3].imshow((x_t[0, 0] + 1) / 2, cmap='gray')\n",
    "axes[3].set_title('$x_{0.5}$ (midpoint)', fontsize=12)\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.suptitle('Velocity Field: The Direction from Data to Noise', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVelocity field interpretation:\")\n",
    "print(\"• Red regions: noise is brighter than data (positive velocity)\")\n",
    "print(\"• Blue regions: noise is darker than data (negative velocity)\")\n",
    "print(\"• White regions: similar values (near-zero velocity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-velocity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: velocity is constant at all timesteps\n",
    "print(\"Verifying velocity is constant along the path...\")\n",
    "print()\n",
    "\n",
    "for t in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    t_tensor = torch.tensor([t])\n",
    "    _, v = flow.forward_process(x_0, x_1, t_tensor)\n",
    "    v_norm = torch.norm(v).item()\n",
    "    print(f\"t = {t:.2f}: ||v|| = {v_norm:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"✓ The velocity norm is identical at all timesteps!\")\n",
    "print(\"  This confirms v = x₁ - x₀ is constant (independent of t).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-intro",
   "metadata": {},
   "source": [
    "## 4. The Neural Network Architecture\n",
    "\n",
    "We need a neural network $v_\\theta(x_t, t)$ that takes:\n",
    "- $x_t$: The noised image (28×28×1)\n",
    "- $t$: The timestep (scalar in [0, 1])\n",
    "\n",
    "And predicts the velocity field $v$ (same shape as $x_t$).\n",
    "\n",
    "### U-Net Architecture\n",
    "\n",
    "We use a **U-Net**, a classic architecture for image-to-image tasks:\n",
    "\n",
    "```\n",
    "Input (28×28) ──┐                      ┌── Output (28×28)\n",
    "                ▼                      ▲\n",
    "            [Encoder]              [Decoder]\n",
    "                │                      │\n",
    "           downsample              upsample\n",
    "                │                      │\n",
    "                ▼                      ▲\n",
    "            (14×14)    ──────────>  (14×14)  ← skip connection\n",
    "                │                      │\n",
    "           downsample              upsample\n",
    "                │                      │\n",
    "                ▼                      ▲\n",
    "             (7×7)     ──────────>   (7×7)   ← skip connection\n",
    "                │                      │\n",
    "                └─────> bottleneck ────┘\n",
    "```\n",
    "\n",
    "**Skip connections** allow fine details to flow directly to the output.\n",
    "\n",
    "### Timestep Conditioning\n",
    "\n",
    "The network needs to know the current timestep $t$ to predict the appropriate velocity. We use **sinusoidal positional encoding** (from the Transformer paper):\n",
    "\n",
    "$$\\text{emb}(t) = \\left[\\sin(\\omega_1 t), \\cos(\\omega_1 t), \\sin(\\omega_2 t), \\cos(\\omega_2 t), \\ldots\\right]$$\n",
    "\n",
    "where $\\omega_i$ are different frequencies. This rich embedding is then added to the network's feature maps at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.models import SimpleUNet\n",
    "\n",
    "model = SimpleUNet(\n",
    "    in_channels=1,       # Grayscale images\n",
    "    model_channels=64,   # Base channel count (doubled at each level)\n",
    "    time_emb_dim=128,    # Timestep embedding dimension\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"\\nThis is a relatively small model - larger models give better results.\")\n",
    "\n",
    "# Test forward pass\n",
    "test_x = torch.randn(4, 1, 28, 28, device=device)\n",
    "test_t = torch.rand(4, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(test_x, test_t)\n",
    "\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"  Input shape:  {test_x.shape}  (batch, channels, height, width)\")\n",
    "print(f\"  Timestep:     {test_t.shape}  (batch,)\")\n",
    "print(f\"  Output shape: {test_out.shape}  (same as input - predicts velocity at each pixel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-intro",
   "metadata": {},
   "source": [
    "## 5. Training: Learning the Velocity Field\n",
    "\n",
    "### The Training Algorithm\n",
    "\n",
    "For each training step:\n",
    "\n",
    "1. **Sample data**: $x_0 \\sim p_{\\text{data}}$ (a batch of real digits)\n",
    "2. **Sample noise**: $x_1 \\sim \\mathcal{N}(0, I)$ (same shape as $x_0$)\n",
    "3. **Sample timestep**: $t \\sim \\text{Uniform}[0, 1]$\n",
    "4. **Compute interpolation**: $x_t = (1-t) x_0 + t x_1$\n",
    "5. **Compute true velocity**: $v = x_1 - x_0$\n",
    "6. **Predict velocity**: $\\hat{v} = v_\\theta(x_t, t)$\n",
    "7. **Compute loss**: $\\mathcal{L} = \\|\\hat{v} - v\\|^2$ (MSE)\n",
    "8. **Update weights**: Backpropagation and gradient descent\n",
    "\n",
    "### Why Does This Work?\n",
    "\n",
    "The model sees many $(x_t, t) \\to v$ examples:\n",
    "- Different data points $x_0$\n",
    "- Different noise samples $x_1$  \n",
    "- Different timesteps $t$\n",
    "\n",
    "It learns to recognize patterns:\n",
    "- \"At $t \\approx 1$ (mostly noise), the velocity points toward structure\"\n",
    "- \"At $t \\approx 0$ (mostly data), the velocity is small adjustments\"\n",
    "\n",
    "The key insight: by learning the **conditional expectation** of velocity given $x_t$ and $t$, the model implicitly learns the marginal distribution $p_t$ at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.train import Trainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    lr=1e-4,          # Learning rate\n",
    "    weight_decay=0.01, # Regularization\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Train\n",
    "NUM_EPOCHS = 30  # Increase to 50+ for better quality\n",
    "\n",
    "print(\"Training the velocity prediction network...\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {train_loader.batch_size}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print()\n",
    "\n",
    "losses = trainer.train(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, marker='o', markersize=4)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('Training Loss: $\\mathcal{L} = \\|v_\\\\theta(x_t, t) - (x_1 - x_0)\\|^2$', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(\"\\nLower loss = model better predicts the velocity field.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sampling-intro",
   "metadata": {},
   "source": [
    "## 6. Sampling: Generating New Images\n",
    "\n",
    "Now for the exciting part - using our trained model to generate new digits!\n",
    "\n",
    "### The Sampling ODE\n",
    "\n",
    "We solve the ODE:\n",
    "$$\\frac{dx}{dt} = v_\\theta(x, t)$$\n",
    "\n",
    "**Starting point**: $x_1 \\sim \\mathcal{N}(0, I)$ (random noise at $t=1$)\n",
    "\n",
    "**Goal**: Integrate backward to $t=0$ to get a sample from $p_{\\text{data}}$\n",
    "\n",
    "### Euler Integration\n",
    "\n",
    "We discretize time into $N$ steps and use the Euler method:\n",
    "\n",
    "$$x_{t - \\Delta t} = x_t - \\Delta t \\cdot v_\\theta(x_t, t)$$\n",
    "\n",
    "where $\\Delta t = 1/N$.\n",
    "\n",
    "**Intuition**: At each step, we:\n",
    "1. Ask the model: \"What velocity should this sample have?\"\n",
    "2. Move in the opposite direction (since we're going backward in time)\n",
    "3. Repeat until we reach $t=0$\n",
    "\n",
    "### Why Backward?\n",
    "\n",
    "During training, the velocity field points from data to noise ($x_1 - x_0$).\n",
    "\n",
    "During sampling, we want to go from noise to data, so we:\n",
    "- Start at $t=1$ (noise)\n",
    "- Subtract the velocity (go opposite direction)\n",
    "- End at $t=0$ (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.sampling import sample\n",
    "\n",
    "# Generate samples\n",
    "model.eval()\n",
    "\n",
    "print(\"Generating 64 new digits from random noise...\")\n",
    "print(\"(Starting at t=1, integrating backward to t=0)\")\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated, trajectory = sample(\n",
    "        model=model,\n",
    "        num_samples=64,\n",
    "        image_shape=(1, 28, 28),\n",
    "        num_steps=50,      # Number of Euler steps\n",
    "        device=device,\n",
    "        return_trajectory=True,\n",
    "    )\n",
    "\n",
    "show_images(generated, nrow=8, title=\"Generated Samples from $p_{\\\\theta}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trajectory-intro",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Generation Process\n",
    "\n",
    "Let's watch how noise transforms into digits step by step.\n",
    "\n",
    "This visualization shows the ODE integration:\n",
    "- $t=1.00$: Pure noise $x_1 \\sim \\mathcal{N}(0, I)$\n",
    "- $t \\to 0$: Following $-v_\\theta(x, t)$ backward\n",
    "- $t=0.00$: Generated digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the trajectory for a few samples\n",
    "num_to_show = 4\n",
    "steps_to_show = [0, 5, 10, 20, 30, 40, 50]  # Which steps to visualize\n",
    "\n",
    "fig, axes = plt.subplots(num_to_show, len(steps_to_show), figsize=(14, 8))\n",
    "\n",
    "for row in range(num_to_show):\n",
    "    for col, step_idx in enumerate(steps_to_show):\n",
    "        img = trajectory[step_idx][row, 0]\n",
    "        img = (img + 1) / 2  # Denormalize\n",
    "        axes[row, col].imshow(img.cpu().numpy(), cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "        if row == 0:\n",
    "            t_val = 1.0 - step_idx / 50\n",
    "            axes[row, col].set_title(f'$t={t_val:.2f}$', fontsize=11)\n",
    "\n",
    "plt.suptitle('Generation Process: Solving $dx/dt = v_\\\\theta(x, t)$ backward from $t=1$ to $t=0$', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserve how structure emerges:\")\n",
    "print(\"• t≈1: Random noise\")\n",
    "print(\"• t≈0.7: Large-scale structure appears (rough digit shape)\")\n",
    "print(\"• t≈0.3: Details emerge (strokes, curves)\")\n",
    "print(\"• t=0: Final digit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-intro",
   "metadata": {},
   "source": [
    "## 8. Analyzing What the Model Learned\n",
    "\n",
    "Let's verify our model learned meaningful velocity predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-velocities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted vs true velocity\n",
    "x_0 = sample_batch[0:1].to(device)\n",
    "x_1 = torch.randn_like(x_0)\n",
    "t = torch.tensor([0.5], device=device)\n",
    "\n",
    "# True velocity\n",
    "true_v = x_1 - x_0\n",
    "\n",
    "# Interpolated sample\n",
    "x_t = (1 - t.view(-1, 1, 1, 1)) * x_0 + t.view(-1, 1, 1, 1) * x_1\n",
    "\n",
    "# Predicted velocity\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_v = model(x_t, t)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Top row: the samples\n",
    "axes[0, 0].imshow((x_0[0, 0].cpu() + 1) / 2, cmap='gray')\n",
    "axes[0, 0].set_title('$x_0$ (data)', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow((x_t[0, 0].cpu() + 1) / 2, cmap='gray')\n",
    "axes[0, 1].set_title('$x_t$ at $t=0.5$\\n(model input)', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow((x_1[0, 0].cpu() + 1) / 2, cmap='gray')\n",
    "axes[0, 2].set_title('$x_1$ (noise)', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Bottom row: velocities\n",
    "vmin, vmax = -2, 2\n",
    "\n",
    "im = axes[1, 0].imshow(true_v[0, 0].cpu(), cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "axes[1, 0].set_title('True $v = x_1 - x_0$', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(pred_v[0, 0].cpu(), cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "axes[1, 1].set_title('Predicted $v_\\\\theta(x_t, t)$', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "error = (pred_v - true_v)[0, 0].cpu()\n",
    "axes[1, 2].imshow(error, cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[1, 2].set_title(f'Error (MSE={torch.mean(error**2):.4f})', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, :], orientation='horizontal', fraction=0.05, pad=0.1)\n",
    "plt.suptitle('Velocity Prediction Analysis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: The model sees only x_t and t, but must predict v = x_1 - x_0.\")\n",
    "print(\"It can't know the exact x_1 used, so it predicts the expected velocity.\")\n",
    "print(\"The prediction captures the overall structure even if not pixel-perfect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for use in Phase 2\n",
    "trainer.save_checkpoint(\"phase1_model.pt\")\n",
    "print(\"Model saved to phase1_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary: What We Learned\n",
    "\n",
    "### The Flow Matching Framework\n",
    "\n",
    "1. **The Goal**: Transform noise $\\mathcal{N}(0, I)$ into data samples\n",
    "\n",
    "2. **The Path**: Linear interpolation $x_t = (1-t)x_0 + tx_1$ creates straight paths\n",
    "\n",
    "3. **The Velocity**: $v = x_1 - x_0$ is constant along each path\n",
    "\n",
    "4. **The Training**: Learn $v_\\theta(x_t, t) \\approx v$ via MSE loss\n",
    "\n",
    "5. **The Sampling**: Solve ODE $dx/dt = v_\\theta$ backward from $t=1$ to $t=0$\n",
    "\n",
    "### Key Mathematical Insights\n",
    "\n",
    "| Concept | Insight |\n",
    "|---------|--------|\n",
    "| **Linear paths** | Simplest possible interpolation, constant velocity |\n",
    "| **MSE loss** | Directly optimizes velocity prediction accuracy |\n",
    "| **Euler integration** | Simple discretization of the sampling ODE |\n",
    "| **No stochastic terms** | Unlike DDPM, flow matching is fully deterministic |\n",
    "\n",
    "### Limitations of This Phase\n",
    "\n",
    "- **Unconditional**: We can't control which digit gets generated\n",
    "- **Simple CNN**: U-Net works but doesn't scale as well as transformers\n",
    "- **Blurry results**: More training and better architecture would help\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Phase 2**: Replace the CNN with a **Diffusion Transformer (DiT)**\n",
    "- Patchify images into sequences\n",
    "- Self-attention for global receptive field\n",
    "- Adaptive layer norm (adaLN) for better timestep conditioning\n",
    "\n",
    "**Phase 3**: Add **class conditioning** to control which digit we generate\n",
    "- Class embeddings\n",
    "- Classifier-free guidance (CFG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
