{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Phase 3: Class-Conditional Generation with Classifier-Free Guidance\n\nIn Phases 1 and 2, we built generative models that produce **random** digits. We had no control over *which* digit gets generated - the model samples from the entire data distribution.\n\nIn this notebook, we add **class conditioning** - the ability to say \"generate a 7\" and actually get a 7!\n\n## The Conditional Generation Problem\n\nMathematically, we're moving from **unconditional** to **conditional** generation:\n\n| Phase | Distribution | Meaning |\n|-------|-------------|---------|\n| 1-2 | $p(x)$ | Generate any digit |\n| 3 | $p(x \\mid y)$ | Generate digit of class $y$ |\n\nThe key insight: instead of learning the marginal distribution $p(x)$, we learn the **conditional distribution** $p(x \\mid y)$ where $y \\in \\{0, 1, \\ldots, 9\\}$ is the digit class.\n\n## From Class Labels to Text Prompts\n\nClass conditioning is a **stepping stone** to text conditioning. The principles are identical:\n\n| Application | Condition $y$ | Distribution |\n|-------------|---------------|--------------|\n| This notebook | Digit class (0-9) | $p(x \\mid y = \\text{digit})$ |\n| Stable Diffusion | Text prompt | $p(x \\mid y = \\text{prompt})$ |\n| DALL-E | Text description | $p(x \\mid y = \\text{text})$ |\n\n## Mathematical Framework\n\nWe modify our flow matching objective to be conditioned on class $y$:\n\n**Unconditional (Phase 1-2):**\n$$\\mathcal{L} = \\mathbb{E}_{x_0, x_1, t} \\left[ \\| v_\\theta(x_t, t) - (x_1 - x_0) \\|^2 \\right]$$\n\n**Conditional (Phase 3):**\n$$\\mathcal{L} = \\mathbb{E}_{x_0, x_1, t, y} \\left[ \\| v_\\theta(x_t, t, y) - (x_1 - x_0) \\|^2 \\right]$$\n\nThe velocity field $v_\\theta$ now takes the class label $y$ as an additional input.\n\n## What We'll Learn\n\n1. **Class Embeddings**: Convert discrete labels to continuous vectors via $e: \\{0, \\ldots, 9\\} \\to \\mathbb{R}^D$\n2. **Conditioning Mechanism**: How to inject class information into the model\n3. **Classifier-Free Guidance (CFG)**: Amplify conditioning at inference time\n4. **Label Dropout**: Training technique that enables CFG"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Auto-reload modules during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up device\n",
    "from opticus import get_device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class-embed-intro",
   "metadata": {},
   "source": "## 1. Class Embeddings: From Discrete Labels to Continuous Vectors\n\nOur class labels are integers: $y \\in \\{0, 1, 2, \\ldots, 9\\}$. But neural networks operate on continuous vectors. How do we bridge this gap?\n\n### The Embedding Function\n\nWe define an **embedding function** $e: \\mathcal{Y} \\to \\mathbb{R}^D$ that maps each class to a learnable vector:\n\n$$e(y) = W_e[y] \\in \\mathbb{R}^D$$\n\nwhere:\n- $\\mathcal{Y} = \\{0, 1, \\ldots, K-1\\}$ is the set of $K$ classes\n- $W_e \\in \\mathbb{R}^{K \\times D}$ is a learnable **embedding matrix**\n- $W_e[y]$ denotes the $y$-th row of $W_e$\n\nFor MNIST with $K=10$ classes and embedding dimension $D=1024$:\n\n$$W_e = \\begin{bmatrix} \nâ€” & e_0 & â€” \\\\\nâ€” & e_1 & â€” \\\\\n& \\vdots & \\\\\nâ€” & e_9 & â€”\n\\end{bmatrix} \\in \\mathbb{R}^{10 \\times 1024}$$\n\nEach row $e_i$ is the learnable embedding for digit $i$.\n\n### Why Embeddings Work\n\nEmbedding tables are universal in deep learning (word embeddings, token embeddings, etc.). They work because:\n\n1. **Gradient Flow**: Each $e_i$ receives gradients only when class $i$ is used, learning class-specific features\n2. **Representation Learning**: The network learns to place similar classes near each other in embedding space\n3. **Flexibility**: Unlike one-hot encodings, embeddings can capture relationships between classes\n\n### The Null Class: Enabling Classifier-Free Guidance\n\nFor CFG (explained in Section 3), we need a special **null class** $\\emptyset$ representing \"no conditioning\":\n\n$$\\mathcal{Y}_{\\text{extended}} = \\{0, 1, \\ldots, K-1, \\emptyset\\}$$\n\nThis gives us $K+1 = 11$ embeddings total. The null embedding $e_\\emptyset$ is learned during training and represents the \"unconditional\" case.\n\n**Implementation**: We use index $K$ (i.e., 10) for the null class:\n\n$$e_\\emptyset = W_e[K] = W_e[10]$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-class-embed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.dit import ClassEmbedding\n",
    "\n",
    "# Create a class embedding layer\n",
    "num_classes = 10\n",
    "embed_dim = 1024  # Same as cond_dim in our model\n",
    "class_embed = ClassEmbedding(num_classes, embed_dim)\n",
    "\n",
    "print(f\"Embedding table shape: {class_embed.embed.weight.shape}\")\n",
    "print(f\"  - {num_classes + 1} classes (10 digits + 1 null class)\")\n",
    "print(f\"  - {embed_dim} dimensions per class\")\n",
    "\n",
    "# Visualize the initial embeddings\n",
    "with torch.no_grad():\n",
    "    all_embeddings = class_embed.embed.weight.numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Show the embedding matrix\n",
    "im = axes[0].imshow(all_embeddings, aspect='auto', cmap='RdBu')\n",
    "axes[0].set_xlabel('Embedding dimension')\n",
    "axes[0].set_ylabel('Class')\n",
    "axes[0].set_yticks(range(11))\n",
    "axes[0].set_yticklabels([str(i) for i in range(10)] + ['null'])\n",
    "axes[0].set_title('Initial Class Embeddings (random)')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Show similarity matrix\n",
    "similarity = np.corrcoef(all_embeddings)\n",
    "im = axes[1].imshow(similarity, cmap='viridis')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Class')\n",
    "axes[1].set_xticks(range(11))\n",
    "axes[1].set_xticklabels([str(i) for i in range(10)] + ['âˆ…'])\n",
    "axes[1].set_yticks(range(11))\n",
    "axes[1].set_yticklabels([str(i) for i in range(10)] + ['âˆ…'])\n",
    "axes[1].set_title('Class Similarity (before training)')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key Insight:\")\n",
    "print(\"   Before training, all classes have random, uncorrelated embeddings.\")\n",
    "print(\"   After training, similar digits (like 3 and 8) might become more similar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditioning-intro",
   "metadata": {},
   "source": "## 2. Conditioning the DiT: Combining Time and Class\n\nIn Phase 2, our DiT was conditioned only on timestep $t$. Now we add class conditioning. The question: how do we combine these two pieces of information?\n\n### Mathematical Framework\n\nWe have two conditioning signals:\n- **Timestep**: $t \\in [0, 1]$ â†’ embedded as $h_t = \\text{TimeEmbed}(t) \\in \\mathbb{R}^D$\n- **Class**: $y \\in \\{0, \\ldots, 9\\}$ â†’ embedded as $h_y = e(y) \\in \\mathbb{R}^D$\n\nWe need to combine these into a single conditioning vector $c \\in \\mathbb{R}^D$.\n\n### Addition: The Simplest Approach\n\nWe use **element-wise addition**:\n\n$$c = h_t + h_y = \\text{TimeEmbed}(t) + e(y)$$\n\nThis is mathematically elegant because:\n\n1. **Superposition**: The network can learn to disentangle the two signals\n2. **Shared representation space**: Both embeddings live in the same $\\mathbb{R}^D$\n3. **Computational efficiency**: No additional parameters or operations\n\n### Why Addition Works: A Linear Algebra Perspective\n\nConsider the combined conditioning as occupying different \"subspaces\":\n\n$$c = h_t + h_y \\approx P_t h + P_y h$$\n\nwhere $P_t$ and $P_y$ are (implicit) projection matrices. If the model learns orthogonal or near-orthogonal representations for time and class, it can cleanly separate them:\n\n$$\\text{Time information: } \\langle c, u_t \\rangle \\approx \\langle h_t, u_t \\rangle$$\n$$\\text{Class information: } \\langle c, u_y \\rangle \\approx \\langle h_y, u_y \\rangle$$\n\nfor learned direction vectors $u_t$ and $u_y$.\n\n### Integration with adaLN\n\nThe combined conditioning $c$ feeds into adaLN (from Phase 2):\n\n$$\\text{adaLN}(x, c) = \\gamma(c) \\odot \\frac{x - \\mu}{\\sigma} + \\beta(c)$$\n\nwhere $\\gamma(c) = W_\\gamma c + b_\\gamma$ and $\\beta(c) = W_\\beta c + b_\\beta$ are linear projections.\n\nExpanding with our combined conditioning:\n\n$$\\gamma(c) = \\gamma(h_t + h_y) = W_\\gamma h_t + W_\\gamma h_y + b_\\gamma$$\n\nThe network learns to use both time and class information through these modulation parameters.\n\n### Architecture Comparison\n\n```\nPhase 2 (Unconditional)          Phase 3 (Class-Conditional)\n                                 \n   Timestep t                       Timestep t    Class y\n      â”‚                                â”‚            â”‚\n      â–¼                                â–¼            â–¼\n  TimeEmbed                        TimeEmbed    ClassEmbed\n      â”‚                                â”‚            â”‚\n      â–¼                                â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n   h_t âˆˆ â„á´°                                 +\n      â”‚                                      â”‚\n      â–¼                                      â–¼\n   adaLN                                c âˆˆ â„á´°\n      â”‚                                      â”‚\n      â–¼                                      â–¼\n   DiT Blocks                             adaLN\n                                             â”‚\n                                             â–¼\n                                         DiT Blocks\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.dit import TimestepEmbedding, ClassEmbedding\n",
    "\n",
    "# Create embedding modules\n",
    "embed_dim = 256\n",
    "cond_dim = embed_dim * 4  # 1024\n",
    "\n",
    "time_embed = TimestepEmbedding(embed_dim, cond_dim)\n",
    "class_embed = ClassEmbedding(10, cond_dim)\n",
    "\n",
    "# Sample inputs\n",
    "t = torch.tensor([0.3])  # Timestep\n",
    "y = torch.tensor([7])    # Class label (digit 7)\n",
    "\n",
    "with torch.no_grad():\n",
    "    time_cond = time_embed(t)\n",
    "    class_cond = class_embed(y)\n",
    "    combined = time_cond + class_cond\n",
    "\n",
    "print(\"Conditioning Vector Construction:\")\n",
    "print(f\"   Time embedding:   {time_cond.shape} â†’ represents t={t.item():.1f}\")\n",
    "print(f\"   Class embedding:  {class_cond.shape} â†’ represents digit {y.item()}\")\n",
    "print(f\"   Combined:         {combined.shape} â†’ used for adaLN conditioning\")\n",
    "\n",
    "# Visualize the combination\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "\n",
    "axes[0].bar(range(50), time_cond[0, :50].numpy())\n",
    "axes[0].set_title(f'Time Embedding (first 50 dims)\\nt={t.item():.1f}')\n",
    "axes[0].set_xlabel('Dimension')\n",
    "\n",
    "axes[1].bar(range(50), class_cond[0, :50].numpy())\n",
    "axes[1].set_title(f'Class Embedding (first 50 dims)\\ndigit={y.item()}')\n",
    "axes[1].set_xlabel('Dimension')\n",
    "\n",
    "axes[2].bar(range(50), combined[0, :50].numpy())\n",
    "axes[2].set_title('Combined Conditioning\\n(time + class)')\n",
    "axes[2].set_xlabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg-intro",
   "metadata": {},
   "source": "## 3. Classifier-Free Guidance: The Mathematical Foundation\n\nHere's a problem: training with class labels and sampling with class labels often produces \"weak\" conditioning. The model generates vaguely correct digits, but they don't really \"pop\".\n\n**Classifier-Free Guidance (CFG)** solves this. It's arguably the most important technique in modern conditional generation.\n\n### The Score Function Perspective\n\nTo understand CFG, let's think in terms of **score functions** (gradients of log-probability):\n\n$$s_\\theta(x_t, t, y) = \\nabla_{x_t} \\log p_\\theta(x_t \\mid t, y)$$\n\nIn diffusion/flow models, the velocity field relates to the score:\n\n$$v_\\theta(x_t, t, y) \\propto -\\sigma_t \\cdot s_\\theta(x_t, t, y)$$\n\n### Applying Bayes' Theorem\n\nUsing Bayes' rule for the conditional distribution:\n\n$$\\log p(x \\mid y) = \\log p(y \\mid x) + \\log p(x) - \\log p(y)$$\n\nTaking gradients with respect to $x$:\n\n$$\\nabla_x \\log p(x \\mid y) = \\nabla_x \\log p(y \\mid x) + \\nabla_x \\log p(x)$$\n\nor equivalently:\n\n$$s(x \\mid y) = s(x \\mid y) - s(x) + s(x) = \\underbrace{s(x)}_{\\text{unconditional}} + \\underbrace{(s(x \\mid y) - s(x))}_{\\text{classifier gradient}}$$\n\n### The CFG Insight: Amplifying the Classifier Gradient\n\nCFG amplifies the \"classifier gradient\" term:\n\n$$s_{\\text{CFG}}(x, y, w) = s(x) + w \\cdot (s(x \\mid y) - s(x))$$\n\nwhere $w > 1$ is the **guidance scale**. Simplifying:\n\n$$s_{\\text{CFG}}(x, y, w) = (1 - w) \\cdot s(x) + w \\cdot s(x \\mid y)$$\n\n### CFG Formula for Velocity Fields\n\nTranslating to velocity fields:\n\n$$\\boxed{v_{\\text{CFG}} = v_{\\text{uncond}} + w \\cdot (v_{\\text{cond}} - v_{\\text{uncond}})}$$\n\nwhich can also be written as:\n\n$$v_{\\text{CFG}} = (1 - w) \\cdot v_{\\text{uncond}} + w \\cdot v_{\\text{cond}}$$\n\nWhere:\n- $v_{\\text{cond}} = v_\\theta(x_t, t, y)$: Velocity with class label\n- $v_{\\text{uncond}} = v_\\theta(x_t, t, \\emptyset)$: Velocity with null class\n- $w$: Guidance scale (typically 3-7)\n\n### Intuitive Understanding\n\n| Term | Meaning |\n|------|---------|\n| $v_{\\text{uncond}}$ | \"What does any digit look like at this noise level?\" |\n| $v_{\\text{cond}}$ | \"What does digit $y$ look like at this noise level?\" |\n| $v_{\\text{cond}} - v_{\\text{uncond}}$ | \"What makes this specifically digit $y$?\" |\n| $w \\cdot (\\ldots)$ | \"Amplify the $y$-specific features\" |\n\n### Effect of Guidance Scale\n\n| Scale $w$ | Effect | Mathematical View |\n|-----------|--------|-------------------|\n| $w = 0$ | Pure unconditional | $v_{\\text{CFG}} = v_{\\text{uncond}}$ |\n| $w = 1$ | Pure conditional (no guidance) | $v_{\\text{CFG}} = v_{\\text{cond}}$ |\n| $w = 3\\text{-}5$ | Good balance | Amplified conditioning |\n| $w > 7$ | Over-saturation | Too much class signal, artifacts |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-cfg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cfg_concept():\n",
    "    \"\"\"\n",
    "    Visualize how CFG amplifies the conditional signal.\n",
    "    \"\"\"\n",
    "    # Create synthetic \"velocities\" to illustrate the concept\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Unconditional: generic velocity field\n",
    "    v_uncond = np.random.randn(28, 28) * 0.3\n",
    "    \n",
    "    # Conditional: adds class-specific structure (e.g., \"7\" shape)\n",
    "    v_class_specific = np.zeros((28, 28))\n",
    "    # Horizontal bar at top (the top of a 7)\n",
    "    v_class_specific[5:8, 8:20] = 0.8\n",
    "    # Diagonal stroke\n",
    "    for i in range(15):\n",
    "        v_class_specific[8+i, 18-i:20-i] = 0.8\n",
    "    \n",
    "    v_cond = v_uncond + v_class_specific\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Row 1: The components\n",
    "    im = axes[0, 0].imshow(v_uncond, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, 0].set_title('Unconditional\\nv_uncond', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(v_cond, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, 1].set_title('Conditional\\nv_cond', fontsize=12)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    difference = v_cond - v_uncond\n",
    "    axes[0, 2].imshow(difference, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, 2].set_title('Difference\\nv_cond - v_uncond', fontsize=12)\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[0, 3].imshow(v_class_specific, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, 3].set_title('\"Class-specific\" signal\\n(what makes it a 7)', fontsize=12)\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Row 2: Different CFG scales\n",
    "    scales = [1.0, 3.0, 5.0, 7.0]\n",
    "    \n",
    "    for i, scale in enumerate(scales):\n",
    "        v_guided = v_uncond + scale * difference\n",
    "        axes[1, i].imshow(v_guided, cmap='RdBu', vmin=-2, vmax=2)\n",
    "        axes[1, i].set_title(f'CFG scale = {scale}', fontsize=12)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Classifier-Free Guidance: Amplifying Class-Specific Features', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š CFG Formula: v_guided = v_uncond + scale Ã— (v_cond - v_uncond)\")\n",
    "    print(\"\\n   â€¢ scale=1: Pure conditional (no amplification)\")\n",
    "    print(\"   â€¢ scale>1: Amplifies what the class 'adds' to the prediction\")\n",
    "    print(\"   â€¢ Higher scale = stronger adherence to class, but may reduce quality\")\n",
    "\n",
    "visualize_cfg_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropout-intro",
   "metadata": {},
   "source": "## 4. Label Dropout: Training for Classifier-Free Guidance\n\nFor CFG to work at inference, we need to run the model **twice**: once conditional ($v_{\\text{cond}}$) and once unconditional ($v_{\\text{uncond}}$). But how does a single model learn both behaviors?\n\n### The Solution: Probabilistic Label Dropout\n\nDuring training, we **randomly drop the class label** with probability $p_{\\text{drop}}$:\n\n$$y_{\\text{train}} = \\begin{cases}\ny & \\text{with probability } 1 - p_{\\text{drop}} \\\\\n\\emptyset & \\text{with probability } p_{\\text{drop}}\n\\end{cases}$$\n\nTypically $p_{\\text{drop}} = 0.1$ (10%).\n\n### Mathematical Interpretation\n\nThe training objective becomes a **mixture** of conditional and unconditional losses:\n\n$$\\mathcal{L}_{\\text{CFG}} = (1 - p_{\\text{drop}}) \\cdot \\mathbb{E}[\\|v_\\theta(x_t, t, y) - v_{\\text{target}}\\|^2] + p_{\\text{drop}} \\cdot \\mathbb{E}[\\|v_\\theta(x_t, t, \\emptyset) - v_{\\text{target}}\\|^2]$$\n\nOr equivalently:\n\n$$\\mathcal{L}_{\\text{CFG}} = \\mathbb{E}_{y_{\\text{train}}}[\\|v_\\theta(x_t, t, y_{\\text{train}}) - v_{\\text{target}}\\|^2]$$\n\nwhere $y_{\\text{train}}$ is sampled according to the dropout distribution.\n\n### What the Model Learns\n\n| Training Mode | Fraction | The Model Learns |\n|---------------|----------|------------------|\n| Conditional | 90% | $v_\\theta(x_t, t, y) \\approx \\mathbb{E}[v \\mid x_t, t, y]$ |\n| Unconditional | 10% | $v_\\theta(x_t, t, \\emptyset) \\approx \\mathbb{E}[v \\mid x_t, t]$ |\n\nThe **same model** learns both the conditional and marginal velocity fields!\n\n### The Null Embedding\n\nWhen the label is dropped, we use a learned **null embedding** $e_\\emptyset$:\n\n$$c_{\\text{uncond}} = h_t + e_\\emptyset$$\n\nrather than simply zeroing out the class signal. This allows the model to learn a proper representation for \"no class specified\".\n\n### Why 10% Dropout?\n\nThe choice of $p_{\\text{drop}} = 0.1$ balances two considerations:\n\n1. **Too low** (e.g., 1%): Model rarely sees unconditional examples â†’ poor $v_{\\text{uncond}}$ estimates\n2. **Too high** (e.g., 50%): Model sees fewer conditional examples â†’ weaker $v_{\\text{cond}}$\n\n10% is empirically found to work well across most applications."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-dropout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_label_dropout():\n",
    "    \"\"\"\n",
    "    Show how label dropout works during training.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    batch_size = 20\n",
    "    drop_prob = 0.1\n",
    "    \n",
    "    # Simulate original labels\n",
    "    original_labels = np.random.randint(0, 10, batch_size)\n",
    "    \n",
    "    # Simulate dropout\n",
    "    drop_mask = np.random.rand(batch_size) < drop_prob\n",
    "    labels_after_dropout = original_labels.copy()\n",
    "    labels_after_dropout[drop_mask] = 10  # Null class\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 5))\n",
    "    \n",
    "    # Original labels\n",
    "    colors = plt.cm.tab10(original_labels / 10)\n",
    "    bars = axes[0].bar(range(batch_size), original_labels + 0.5, color=colors, edgecolor='black')\n",
    "    axes[0].set_ylabel('Class Label')\n",
    "    axes[0].set_title('Original Labels', fontsize=12)\n",
    "    axes[0].set_ylim(0, 11)\n",
    "    axes[0].set_yticks(range(11))\n",
    "    axes[0].set_yticklabels([str(i) for i in range(10)] + ['null'])\n",
    "    axes[0].axhline(y=10, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # After dropout\n",
    "    colors_after = []\n",
    "    for i, (orig, after) in enumerate(zip(original_labels, labels_after_dropout)):\n",
    "        if after == 10:  # Dropped\n",
    "            colors_after.append('lightgray')\n",
    "        else:\n",
    "            colors_after.append(plt.cm.tab10(orig / 10))\n",
    "    \n",
    "    bars = axes[1].bar(range(batch_size), labels_after_dropout + 0.5, color=colors_after, edgecolor='black')\n",
    "    # Highlight dropped samples\n",
    "    for i, dropped in enumerate(drop_mask):\n",
    "        if dropped:\n",
    "            axes[1].annotate('DROPPED', (i, 10.5), ha='center', fontsize=8, color='red')\n",
    "    \n",
    "    axes[1].set_xlabel('Sample Index')\n",
    "    axes[1].set_ylabel('Class Label')\n",
    "    axes[1].set_title(f'After {drop_prob*100:.0f}% Label Dropout (gray = null class)', fontsize=12)\n",
    "    axes[1].set_ylim(0, 12)\n",
    "    axes[1].set_yticks(range(11))\n",
    "    axes[1].set_yticklabels([str(i) for i in range(10)] + ['null'])\n",
    "    axes[1].axhline(y=10, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Label Dropout Statistics:\")\n",
    "    print(f\"   â€¢ Total samples: {batch_size}\")\n",
    "    print(f\"   â€¢ Dropped: {drop_mask.sum()} ({drop_mask.mean()*100:.0f}%)\")\n",
    "    print(f\"   â€¢ Kept: {(~drop_mask).sum()} ({(~drop_mask).mean()*100:.0f}%)\")\n",
    "\n",
    "visualize_label_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-intro",
   "metadata": {},
   "source": "## 5. The Conditional DiT Architecture\n\nLet's examine our class-conditional DiT. The changes from Phase 2 are minimal but powerful.\n\n### Forward Pass Comparison\n\n**Phase 2 (Unconditional):**\n$$v_\\theta(x, t) = \\text{DiT}(x, \\text{TimeEmbed}(t))$$\n\n**Phase 3 (Conditional):**\n$$v_\\theta(x, t, y) = \\text{ConditionalDiT}(x, \\text{TimeEmbed}(t) + e(y))$$\n\n### Mathematical Summary\n\n| Component | Equation | Shape |\n|-----------|----------|-------|\n| Time embedding | $h_t = \\text{TimeEmbed}(t)$ | $\\mathbb{R}^D$ |\n| Class embedding | $h_y = e(y) = W_e[y]$ | $\\mathbb{R}^D$ |\n| Combined conditioning | $c = h_t + h_y$ | $\\mathbb{R}^D$ |\n| adaLN modulation | $\\gamma, \\beta = \\text{Linear}(c)$ | $\\mathbb{R}^{d_{model}}$ each |\n\n### Parameter Count\n\nThe only additional parameters are the class embeddings:\n\n$$\\Delta_{\\text{params}} = (K + 1) \\times D = 11 \\times 1024 = 11{,}264$$\n\nwhere:\n- $K = 10$ digit classes\n- $+1$ for the null class\n- $D = 1024$ embedding dimension"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.dit import ConditionalDiT\n",
    "\n",
    "# Create the conditional DiT model\n",
    "model = ConditionalDiT(\n",
    "    num_classes=10,     # 10 digit classes\n",
    "    img_size=28,        # MNIST image size\n",
    "    patch_size=4,       # 4Ã—4 patches â†’ 7Ã—7 = 49 patches\n",
    "    in_channels=1,      # Grayscale\n",
    "    embed_dim=256,      # Embedding dimension\n",
    "    depth=6,            # Number of transformer blocks\n",
    "    num_heads=8,        # Attention heads\n",
    "    mlp_ratio=4.0,      # MLP expansion\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"ConditionalDiT Parameters: {num_params:,}\")\n",
    "\n",
    "# Compare to unconditional DiT\n",
    "from opticus.dit import DiT\n",
    "uncond_model = DiT()\n",
    "uncond_params = sum(p.numel() for p in uncond_model.parameters() if p.requires_grad)\n",
    "print(f\"Unconditional DiT Parameters: {uncond_params:,}\")\n",
    "print(f\"Difference: {num_params - uncond_params:,} (the class embeddings)\")\n",
    "\n",
    "# Test forward pass with different inputs\n",
    "test_x = torch.randn(4, 1, 28, 28, device=device)\n",
    "test_t = torch.rand(4, device=device)\n",
    "test_y = torch.randint(0, 10, (4,), device=device)\n",
    "\n",
    "print(\"\\n--- Testing Forward Pass ---\")\n",
    "\n",
    "# With class labels\n",
    "with torch.no_grad():\n",
    "    out_cond = model(test_x, test_t, test_y)\n",
    "print(f\"With class labels:    input {test_x.shape} â†’ output {out_cond.shape}\")\n",
    "\n",
    "# Without class labels (unconditional)\n",
    "with torch.no_grad():\n",
    "    out_uncond = model(test_x, test_t, y=None)\n",
    "print(f\"Without class labels: input {test_x.shape} â†’ output {out_uncond.shape}\")\n",
    "\n",
    "print(\"\\nâœ“ Model handles both conditional and unconditional forward passes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-intro",
   "metadata": {},
   "source": "## 6. Training the Conditional DiT\n\nThe training loop is nearly identical to Phase 2, with two modifications:\n\n### Conditional Loss Function\n\nFor each training sample $(x_0, y)$ where $x_0$ is an image and $y$ is its class:\n\n1. **Sample noise and time**: $x_1 \\sim \\mathcal{N}(0, I)$, $t \\sim \\text{Uniform}(0, 1)$\n2. **Interpolate**: $x_t = (1-t) x_0 + t x_1$\n3. **Apply label dropout**:\n$$y_{\\text{train}} = \\begin{cases} y & \\text{w.p. } 0.9 \\\\ \\emptyset & \\text{w.p. } 0.1 \\end{cases}$$\n4. **Compute loss**: $\\mathcal{L} = \\| v_\\theta(x_t, t, y_{\\text{train}}) - (x_1 - x_0) \\|^2$\n\n### Algorithm: Conditional Flow Matching Training\n\n```\nInput: Dataset {(x, y)}, model v_Î¸, dropout probability p_drop\nFor each batch:\n    1. Sample (xâ‚€, y) from dataset\n    2. Sample xâ‚ ~ N(0, I), t ~ U(0, 1)\n    3. Compute x_t = (1-t)Â·xâ‚€ + tÂ·xâ‚\n    4. Compute target v = xâ‚ - xâ‚€\n    5. With probability p_drop: y â† âˆ… (null class)\n    6. Predict vÌ‚ = v_Î¸(x_t, t, y)\n    7. Loss = ||vÌ‚ - v||Â²\n    8. Backpropagate and update Î¸\n```\n\nThe `ConditionalTrainer` class implements this with the label dropout handled automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST with labels\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Show sample data\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Labels: {labels[:10].tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.train import ConditionalTrainer\n",
    "\n",
    "# Create the conditional trainer\n",
    "trainer = ConditionalTrainer(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    label_drop_prob=0.1,  # 10% label dropout for CFG\n",
    "    num_classes=10,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Training Conditional DiT with CFG label dropout...\")\n",
    "print(\"(10% of samples trained without class labels)\\n\")\n",
    "\n",
    "NUM_EPOCHS = 30  # Increase for better results\n",
    "losses = trainer.train(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Conditional DiT Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sampling-intro",
   "metadata": {},
   "source": "## 7. Sampling with Classifier-Free Guidance\n\nNow for the exciting part - generating specific digits with controllable guidance!\n\n### CFG Sampling Algorithm\n\nAt each step of the ODE solver, we apply CFG:\n\n**Algorithm: CFG-Guided Euler Sampling**\n\n```\nInput: Trained model v_Î¸, target class y, guidance scale w, num_steps N\nx_t â† sample from N(0, I)    # Start with pure noise\ndt = 1/N                      # Step size\n\nFor t = 1, 1-dt, 1-2dt, ..., dt:\n    # Get conditional velocity (with class)\n    v_cond = v_Î¸(x_t, t, y)\n    \n    # Get unconditional velocity (with null class âˆ…)\n    v_uncond = v_Î¸(x_t, t, âˆ…)\n    \n    # Apply CFG formula\n    v_guided = v_uncond + w Â· (v_cond - v_uncond)\n    \n    # Euler step (backward in time)\n    x_t â† x_t - dt Â· v_guided\n\nReturn x_0    # Generated image\n```\n\n### Computational Cost\n\nCFG requires **two forward passes** per sampling step:\n\n| Method | Forward Passes per Step | Total for 50 Steps |\n|--------|------------------------|-------------------|\n| Unconditional | 1 | 50 |\n| Conditional (no CFG) | 1 | 50 |\n| Conditional + CFG | 2 | 100 |\n\nThis 2Ã— cost is why CFG is sometimes called \"expensive\" - but the quality improvement is worth it.\n\n### Implementation Note\n\nIn practice, we can batch the two forward passes together for efficiency:\n\n```python\n# Efficient: batch conditional and unconditional together\nx_batch = torch.cat([x_t, x_t])           # [2B, C, H, W]\ny_batch = torch.cat([y, null_class])       # [2B]\nv_batch = model(x_batch, t, y_batch)       # Single forward pass\nv_cond, v_uncond = v_batch.chunk(2)        # Split results\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.sampling import sample_conditional, sample_each_class\n",
    "\n",
    "def show_images(images, nrow=8, title=\"\"):\n",
    "    \"\"\"Display a grid of images.\"\"\"\n",
    "    images = (images + 1) / 2  # Denormalize\n",
    "    images = images.clamp(0, 1)\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow, padding=2)\n",
    "    plt.figure(figsize=(12, 12 * grid.shape[1] / grid.shape[2]))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Generate samples for each class\n",
    "model.eval()\n",
    "\n",
    "print(\"Generating 4 samples for each digit (0-9) with CFG scale=4.0...\")\n",
    "\n",
    "samples = sample_each_class(\n",
    "    model=model,\n",
    "    num_per_class=4,\n",
    "    image_shape=(1, 28, 28),\n",
    "    num_steps=50,\n",
    "    cfg_scale=4.0,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Reshape for display: (40, 1, 28, 28) â†’ (10, 4, 1, 28, 28)\n",
    "samples_grid = samples.view(10, 4, 1, 28, 28)\n",
    "\n",
    "# Create a nice display\n",
    "fig, axes = plt.subplots(10, 4, figsize=(8, 20))\n",
    "\n",
    "for digit in range(10):\n",
    "    for i in range(4):\n",
    "        img = (samples_grid[digit, i, 0] + 1) / 2\n",
    "        axes[digit, i].imshow(img.cpu().numpy(), cmap='gray')\n",
    "        axes[digit, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[digit, i].set_ylabel(f'{digit}', rotation=0, fontsize=14, labelpad=20)\n",
    "        if digit == 0:\n",
    "            axes[digit, i].set_title(f'Sample {i+1}')\n",
    "\n",
    "plt.suptitle('Class-Conditional Generation: \"Generate digit X\"', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg-comparison-intro",
   "metadata": {},
   "source": "## 8. Effect of Guidance Scale\n\nThe guidance scale $w$ controls the **trade-off between sample quality and diversity**:\n\n### Mathematical Interpretation\n\nRecall the CFG formula:\n\n$$v_{\\text{CFG}} = v_{\\text{uncond}} + w \\cdot (v_{\\text{cond}} - v_{\\text{uncond}})$$\n\n- **Low $w$**: Samples closer to the learned distribution â†’ more diversity, less class fidelity\n- **High $w$**: Samples pushed toward class-specific regions â†’ less diversity, higher class fidelity\n\n### Quality-Diversity Trade-off\n\nThis is formalized as:\n\n$$\\text{High } w \\implies \\text{Low } H(x \\mid y) \\implies \\text{Strong conditioning}$$\n\nwhere $H(x \\mid y)$ is the conditional entropy of generated samples given class $y$.\n\nLet's visualize this trade-off:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cfg_scales(model, target_digit=7, scales=[1.0, 2.0, 3.0, 4.0, 5.0, 7.0]):\n",
    "    \"\"\"\n",
    "    Compare generation quality across different CFG scales.\n",
    "    \"\"\"\n",
    "    num_samples = 6\n",
    "    \n",
    "    # Use same random seed for fair comparison\n",
    "    torch.manual_seed(42)\n",
    "    initial_noise = torch.randn(num_samples, 1, 28, 28, device=device)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(scales), num_samples, figsize=(12, 2*len(scales)))\n",
    "    \n",
    "    for row, scale in enumerate(scales):\n",
    "        # Reset to same noise\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        labels = torch.full((num_samples,), target_digit, dtype=torch.long)\n",
    "        samples = sample_conditional(\n",
    "            model=model,\n",
    "            class_labels=labels,\n",
    "            image_shape=(1, 28, 28),\n",
    "            num_steps=50,\n",
    "            cfg_scale=scale,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        for col in range(num_samples):\n",
    "            img = (samples[col, 0] + 1) / 2\n",
    "            axes[row, col].imshow(img.cpu().numpy(), cmap='gray')\n",
    "            axes[row, col].axis('off')\n",
    "            if col == 0:\n",
    "                axes[row, col].set_ylabel(f'scale={scale}', fontsize=11)\n",
    "    \n",
    "    plt.suptitle(f'Effect of CFG Scale on Generating Digit \"{target_digit}\"', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š CFG Scale Analysis:\")\n",
    "    print(\"   â€¢ scale=1.0: No guidance (pure conditional)\")\n",
    "    print(\"   â€¢ scale=2-3: Light guidance, maintains diversity\")\n",
    "    print(\"   â€¢ scale=4-5: Strong guidance, clear digit identity\")\n",
    "    print(\"   â€¢ scale=7+: Very strong, may oversaturate/distort\")\n",
    "\n",
    "compare_cfg_scales(model, target_digit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-digits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generation for different digits at optimal CFG scale\n",
    "print(\"Generating 8 samples for each digit with CFG scale=4.0...\\n\")\n",
    "\n",
    "samples_all = sample_each_class(\n",
    "    model=model,\n",
    "    num_per_class=8,\n",
    "    image_shape=(1, 28, 28),\n",
    "    num_steps=50,\n",
    "    cfg_scale=4.0,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Show as 10 rows (one per digit), 8 columns (samples)\n",
    "show_images(samples_all, nrow=8, title='Class-Conditional Generation (CFG=4.0)')\n",
    "\n",
    "print(\"Each row is a different digit class (0-9)\")\n",
    "print(\"Each column is a different sample from that class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uncond-comparison-intro",
   "metadata": {},
   "source": "## 9. Conditional vs Unconditional Comparison\n\nLet's directly compare three sampling modes for the same target class:\n\n| Mode | Class Input | CFG Scale | Distribution Sampled |\n|------|-------------|-----------|---------------------|\n| Unconditional | $\\emptyset$ | N/A | $p(x)$ |\n| Conditional (no CFG) | $y$ | $w=1$ | $p(x \\mid y)$ |\n| Conditional + CFG | $y$ | $w>1$ | \"Sharpened\" $p(x \\mid y)$ |\n\nThe CFG-guided samples should show the strongest class identity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uncond-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.sampling import sample\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "target_class = 3  # Generate threes\n",
    "num_samples = 16\n",
    "\n",
    "# 1. Unconditional (no class)\n",
    "# Use null class for all samples\n",
    "null_labels = torch.full((num_samples,), 10, dtype=torch.long, device=device)  # 10 = null\n",
    "uncond_samples = sample_conditional(\n",
    "    model=model,\n",
    "    class_labels=null_labels,\n",
    "    image_shape=(1, 28, 28),\n",
    "    cfg_scale=1.0,  # No guidance\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 2. Conditional without CFG\n",
    "torch.manual_seed(123)\n",
    "cond_nocfg_samples = sample_conditional(\n",
    "    model=model,\n",
    "    class_labels=[target_class] * num_samples,\n",
    "    image_shape=(1, 28, 28),\n",
    "    cfg_scale=1.0,  # No amplification\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 3. Conditional with CFG\n",
    "torch.manual_seed(123)\n",
    "cond_cfg_samples = sample_conditional(\n",
    "    model=model,\n",
    "    class_labels=[target_class] * num_samples,\n",
    "    image_shape=(1, 28, 28),\n",
    "    cfg_scale=4.0,  # Strong guidance\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "for ax, samples, title in [\n",
    "    (axes[0], uncond_samples, 'Unconditional (null class)'),\n",
    "    (axes[1], cond_nocfg_samples, f'Conditional (class={target_class}) without CFG'),\n",
    "    (axes[2], cond_cfg_samples, f'Conditional (class={target_class}) with CFG scale=4.0'),\n",
    "]:\n",
    "    grid = torchvision.utils.make_grid((samples + 1) / 2, nrow=16, padding=2)\n",
    "    ax.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Comparison for generating '{target_class}':\")\n",
    "print(\"   â€¢ Unconditional: Random digits (no control)\")\n",
    "print(f\"   â€¢ Conditional (no CFG): Tends toward {target_class} but may be weak\")\n",
    "print(f\"   â€¢ Conditional (CFG=4): Strong {target_class} identity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class-embed-learned-intro",
   "metadata": {},
   "source": "## 10. Learned Embedding Structure\n\nAfter training, the class embeddings $\\{e_0, e_1, \\ldots, e_9, e_\\emptyset\\}$ should exhibit learned structure.\n\n### Expected Properties\n\n1. **Semantic Clustering**: Similar digits (e.g., 3 and 8, 4 and 9) might have similar embeddings\n2. **Null Separation**: The null embedding $e_\\emptyset$ should be distinct (represents \"any digit\")\n3. **Discriminability**: Different classes should be distinguishable in embedding space\n\n### Analysis Metrics\n\nWe compute:\n- **Cosine similarity**: $\\text{sim}(e_i, e_j) = \\frac{e_i \\cdot e_j}{\\|e_i\\| \\|e_j\\|}$\n- **PCA projection**: Reduce to 2D for visualization\n\nNote: The structure emerges purely from the training objective - no explicit clustering loss is used."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "class-embed-learned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned class embeddings\n",
    "with torch.no_grad():\n",
    "    learned_embeddings = model.class_embed.embed.weight.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Embedding matrix\n",
    "im = axes[0].imshow(learned_embeddings, aspect='auto', cmap='RdBu')\n",
    "axes[0].set_xlabel('Embedding dimension')\n",
    "axes[0].set_ylabel('Class')\n",
    "axes[0].set_yticks(range(11))\n",
    "axes[0].set_yticklabels([str(i) for i in range(10)] + ['null'])\n",
    "axes[0].set_title('Learned Class Embeddings')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# 2. Similarity matrix\n",
    "similarity = np.corrcoef(learned_embeddings)\n",
    "im = axes[1].imshow(similarity, cmap='viridis', vmin=-1, vmax=1)\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Class')\n",
    "axes[1].set_xticks(range(11))\n",
    "axes[1].set_xticklabels([str(i) for i in range(10)] + ['âˆ…'])\n",
    "axes[1].set_yticks(range(11))\n",
    "axes[1].set_yticklabels([str(i) for i in range(10)] + ['âˆ…'])\n",
    "axes[1].set_title('Class Similarity (after training)')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "# 3. 2D PCA projection\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(learned_embeddings)\n",
    "\n",
    "for i in range(10):\n",
    "    axes[2].scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], s=200, \n",
    "                   c=[plt.cm.tab10(i)], edgecolors='black', linewidths=2)\n",
    "    axes[2].annotate(str(i), (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    fontsize=14, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Plot null class differently\n",
    "axes[2].scatter(embeddings_2d[10, 0], embeddings_2d[10, 1], s=200,\n",
    "               c='lightgray', edgecolors='black', linewidths=2, marker='s')\n",
    "axes[2].annotate('âˆ…', (embeddings_2d[10, 0], embeddings_2d[10, 1]),\n",
    "                fontsize=14, ha='center', va='center', color='black')\n",
    "\n",
    "axes[2].set_xlabel('PCA Component 1')\n",
    "axes[2].set_ylabel('PCA Component 2')\n",
    "axes[2].set_title('Class Embeddings (PCA Projection)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Learned Embedding Analysis:\")\n",
    "print(\"   â€¢ Similarity matrix shows which digits the model considers 'similar'\")\n",
    "print(\"   â€¢ PCA projection shows the embedding space structure\")\n",
    "print(\"   â€¢ The null class (âˆ…) should be somewhat central/neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary: Key Mathematical Concepts\n\n### What We Built\n\nWe extended the DiT from Phase 2 with **class conditioning** and **Classifier-Free Guidance**.\n\n### Key Equations Summary\n\n| Concept | Equation |\n|---------|----------|\n| Class embedding | $e(y) = W_e[y] \\in \\mathbb{R}^D$ |\n| Combined conditioning | $c = \\text{TimeEmbed}(t) + e(y)$ |\n| Label dropout (training) | $y_{\\text{train}} \\sim \\text{Bernoulli}(1-p_{\\text{drop}}) \\cdot y + \\text{Bernoulli}(p_{\\text{drop}}) \\cdot \\emptyset$ |\n| Training loss | $\\mathcal{L} = \\mathbb{E}\\left[\\|v_\\theta(x_t, t, y_{\\text{train}}) - (x_1 - x_0)\\|^2\\right]$ |\n| **CFG formula** | $\\boxed{v_{\\text{CFG}} = v_{\\text{uncond}} + w \\cdot (v_{\\text{cond}} - v_{\\text{uncond}})}$ |\n| Alternative CFG form | $v_{\\text{CFG}} = (1-w) \\cdot v_{\\text{uncond}} + w \\cdot v_{\\text{cond}}$ |\n\n### Classifier-Free Guidance Derivation\n\nStarting from Bayes' rule:\n$$\\nabla_x \\log p(x \\mid y) = \\nabla_x \\log p(x) + \\nabla_x \\log p(y \\mid x)$$\n\nThe classifier gradient term $\\nabla_x \\log p(y \\mid x)$ is implicitly learned as:\n$$\\nabla_x \\log p(y \\mid x) \\propto v_{\\text{cond}} - v_{\\text{uncond}}$$\n\nCFG amplifies this by factor $w$:\n$$v_{\\text{CFG}} = v_{\\text{uncond}} + w \\cdot (v_{\\text{cond}} - v_{\\text{uncond}})$$\n\n### Hyperparameter Recommendations\n\n| Parameter | Typical Value | Effect |\n|-----------|--------------|--------|\n| Label dropout $p_{\\text{drop}}$ | 0.1 | Enables CFG; higher = more unconditional training |\n| CFG scale $w$ | 3-5 | Quality/diversity trade-off; higher = stronger conditioning |\n| Embedding dim $D$ | 1024 | Same as conditioning dimension |\n\n### Computational Considerations\n\n| Phase | Model | Forward Passes/Step | Additional Params |\n|-------|-------|---------------------|-------------------|\n| 2 | DiT | 1 | - |\n| 3 | ConditionalDiT | 2 (with CFG) | $(K+1) \\times D$ |\n\n## Looking Ahead: Phase 4\n\nIn Phase 4, we'll move from **class labels** to **text prompts**:\n\n| Phase 3 | Phase 4 |\n|---------|---------|\n| $y \\in \\{0, \\ldots, 9\\}$ | $y = \\text{\"a photo of a cat\"}$ |\n| Embedding table $W_e$ | CLIP text encoder |\n| Addition to timestep | Cross-attention |\n\nThe mathematical principles remain the same - we're just using richer conditioning!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the trained model\n",
    "# trainer.save_checkpoint(\"phase3_conditional_dit.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}