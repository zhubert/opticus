{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Phase 2: Diffusion Transformer (DiT)\n",
    "\n",
    "In Phase 1, we built a flow matching model using a **CNN (U-Net)**. It worked - we generated digits from noise! But the architecture has fundamental limitations.\n",
    "\n",
    "In this notebook, we replace the CNN with a **Diffusion Transformer (DiT)** - the same architecture behind Stable Diffusion 3, Sora, and other state-of-the-art generators.\n",
    "\n",
    "## The Limitations of CNNs\n",
    "\n",
    "CNNs process images through local convolution kernels (typically 3√ó3 or 5√ó5):\n",
    "\n",
    "```\n",
    "CNN Receptive Field Growth:\n",
    "\n",
    "Layer 1: Each pixel sees 3√ó3 = 9 pixels\n",
    "Layer 2: Each pixel sees 5√ó5 = 25 pixels  \n",
    "Layer 3: Each pixel sees 7√ó7 = 49 pixels\n",
    "  ...\n",
    "Layer N: Receptive field grows linearly with depth\n",
    "```\n",
    "\n",
    "**Problem**: To see the whole 28√ó28 image, you need many layers. Information must propagate step-by-step through the network.\n",
    "\n",
    "## Why Transformers?\n",
    "\n",
    "| Aspect | CNN (U-Net) | Transformer (DiT) |\n",
    "|--------|-------------|-------------------|\n",
    "| **Receptive Field** | Local (3√ó3), grows with depth | Global from layer 1 |\n",
    "| **Long-range Dependencies** | Requires many layers | Direct attention |\n",
    "| **Scaling Laws** | Diminishing returns | Predictable improvement |\n",
    "| **Conditioning** | Add/concatenate features | Modulate every operation (adaLN) |\n",
    "\n",
    "The key insight from the [DiT paper](https://arxiv.org/abs/2212.09748): transformers follow **scaling laws**. Double the compute ‚Üí predictably better results. This is why modern image generators use transformers.\n",
    "\n",
    "## What We'll Learn\n",
    "\n",
    "1. **Patchification**: Converting images to sequences (the bridge to transformers)\n",
    "2. **Positional Embeddings**: Encoding 2D spatial structure\n",
    "3. **Self-Attention**: The mathematical heart of transformers\n",
    "4. **Adaptive Layer Norm (adaLN)**: Superior timestep conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up device\n",
    "from opticus import get_device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patchify-theory",
   "metadata": {},
   "source": [
    "## 1. Patchification: Images as Token Sequences\n",
    "\n",
    "Transformers were designed for **sequences** of tokens (words in NLP). To apply them to images, we need to convert the 2D pixel grid into a 1D sequence.\n",
    "\n",
    "### The Mathematical Operation\n",
    "\n",
    "For an image $x \\in \\mathbb{R}^{H \\times W \\times C}$ with patch size $P$:\n",
    "\n",
    "1. **Divide into patches**: Split into $N = \\frac{H}{P} \\times \\frac{W}{P}$ non-overlapping patches\n",
    "2. **Flatten each patch**: Each patch becomes a vector of dimension $P^2 \\cdot C$\n",
    "3. **Project to embedding space**: Linear projection $E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$\n",
    "\n",
    "$$\\text{patches} = \\text{Reshape}(x) \\cdot E \\in \\mathbb{R}^{N \\times D}$$\n",
    "\n",
    "For our MNIST images (28√ó28√ó1) with patch size 4:\n",
    "- $N = \\frac{28}{4} \\times \\frac{28}{4} = 7 \\times 7 = 49$ patches\n",
    "- Each patch: $4 \\times 4 \\times 1 = 16$ pixels\n",
    "- Projected to $D = 256$ dimensions\n",
    "\n",
    "### Why Patches Work\n",
    "\n",
    "| Approach | Sequence Length | Computation |\n",
    "|----------|-----------------|-------------|\n",
    "| Pixel-level | $28 \\times 28 = 784$ tokens | $O(784^2) = 614,656$ |\n",
    "| Patch-level (P=4) | $7 \\times 7 = 49$ tokens | $O(49^2) = 2,401$ |\n",
    "\n",
    "Patches reduce computation by **256√ó** while preserving local structure within each patch.\n",
    "\n",
    "### The Trade-off\n",
    "\n",
    "- **Larger patches**: Fewer tokens, faster, but lose fine detail\n",
    "- **Smaller patches**: More tokens, slower, but preserve detail\n",
    "- **Common choices**: P=8, 14, or 16 for 224√ó224 images; P=4 for our 28√ó28 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "sample_img, label = train_dataset[0]\n",
    "print(f\"Image shape: {sample_img.shape}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-patches",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patchification(img, patch_size=4):\n",
    "    \"\"\"\n",
    "    Visualize the patchification process with mathematical annotations.\n",
    "    \"\"\"\n",
    "    img_display = (img[0] + 1) / 2  # Denormalize\n",
    "    H, W = img_display.shape\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "    N = num_patches_h * num_patches_w\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img_display.numpy(), cmap='gray')\n",
    "    axes[0].set_title(f'Original: $x \\\\in \\\\mathbb{{R}}^{{{H}√ó{W}√ó1}}$', fontsize=11)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Image with patch grid\n",
    "    axes[1].imshow(img_display.numpy(), cmap='gray')\n",
    "    for i in range(1, num_patches_h):\n",
    "        axes[1].axhline(y=i * patch_size - 0.5, color='red', linewidth=1)\n",
    "    for j in range(1, num_patches_w):\n",
    "        axes[1].axvline(x=j * patch_size - 0.5, color='red', linewidth=1)\n",
    "    axes[1].set_title(f'Divide: $N = {num_patches_h}√ó{num_patches_w} = {N}$ patches', fontsize=11)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Extract and show patches as sequence\n",
    "    patches = img_display.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size)\n",
    "    patches = patches.reshape(-1, patch_size, patch_size)\n",
    "    \n",
    "    patch_grid = torchvision.utils.make_grid(\n",
    "        patches[:14].unsqueeze(1), nrow=7, padding=1, pad_value=0.5\n",
    "    )\n",
    "    axes[2].imshow(patch_grid[0].numpy(), cmap='gray')\n",
    "    axes[2].set_title(f'Flatten: each patch ‚àà $\\\\mathbb{{R}}^{{{patch_size**2}}}$', fontsize=11)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Show embedding projection conceptually\n",
    "    embed_dim = 256\n",
    "    projection = np.random.randn(patch_size**2, embed_dim) * 0.1\n",
    "    axes[3].imshow(projection, aspect='auto', cmap='RdBu', vmin=-0.3, vmax=0.3)\n",
    "    axes[3].set_xlabel(f'Embedding dim ({embed_dim})')\n",
    "    axes[3].set_ylabel(f'Patch pixels ({patch_size**2})')\n",
    "    axes[3].set_title(f'Project: $E \\\\in \\\\mathbb{{R}}^{{{patch_size**2}√ó{embed_dim}}}$', fontsize=11)\n",
    "    \n",
    "    plt.suptitle('Patchification: Image ‚Üí Sequence of Embeddings', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Patchification Summary:\")\n",
    "    print(f\"   Input:  x ‚àà ‚Ñù^({H}√ó{W}√ó1) = {H*W} values\")\n",
    "    print(f\"   Patches: {N} patches, each {patch_size}√ó{patch_size} = {patch_size**2} pixels\")\n",
    "    print(f\"   Output: {N} tokens, each {embed_dim}-dimensional\")\n",
    "    print(f\"   Computation reduction: {H*W}¬≤ ‚Üí {N}¬≤ = {(H*W)**2:,} ‚Üí {N**2:,} ({(H*W)**2 // N**2}√ó fewer)\")\n",
    "\n",
    "visualize_patchification(sample_img, patch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pos-embed-theory",
   "metadata": {},
   "source": [
    "## 2. Positional Embeddings: Encoding Spatial Structure\n",
    "\n",
    "When we flatten patches into a sequence, we lose spatial information. The transformer sees:\n",
    "\n",
    "$$[p_1, p_2, p_3, \\ldots, p_{49}]$$\n",
    "\n",
    "But it doesn't know that $p_1$ is top-left and $p_{49}$ is bottom-right!\n",
    "\n",
    "### The Solution: Add Position Information\n",
    "\n",
    "We add a **positional embedding** to each patch embedding:\n",
    "\n",
    "$$z_i = p_i + \\text{PE}(\\text{row}_i, \\text{col}_i)$$\n",
    "\n",
    "### Sinusoidal Positional Encoding\n",
    "\n",
    "We use sinusoidal functions at different frequencies (from \"Attention Is All You Need\"):\n",
    "\n",
    "$$\\text{PE}(\\text{pos}, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d}}\\right)$$\n",
    "$$\\text{PE}(\\text{pos}, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "where $i$ is the dimension index and $d$ is the embedding dimension.\n",
    "\n",
    "### Why Sinusoids?\n",
    "\n",
    "1. **Unique encoding**: Each position gets a unique pattern\n",
    "2. **Relative positions**: $\\text{PE}(\\text{pos}+k)$ can be expressed as a linear function of $\\text{PE}(\\text{pos})$\n",
    "3. **Bounded values**: All values in $[-1, 1]$\n",
    "4. **No learned parameters**: Works for any sequence length\n",
    "\n",
    "### 2D Extension for Images\n",
    "\n",
    "For images, we encode both row and column positions:\n",
    "\n",
    "$$\\text{PE}_{2D}(r, c) = [\\text{PE}_{1D}(r), \\text{PE}_{1D}(c)]$$\n",
    "\n",
    "Each position gets a $D$-dimensional vector: $D/2$ dimensions for row, $D/2$ for column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-pos-embed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_positional_embeddings(grid_size=7, embed_dim=256):\n",
    "    \"\"\"\n",
    "    Visualize 2D sinusoidal positional embeddings with mathematical detail.\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # Create 1D sinusoidal embeddings\n",
    "    half_dim = embed_dim // 4  # sin_row, cos_row, sin_col, cos_col\n",
    "    \n",
    "    # Frequency bands: 10000^(-2i/d)\n",
    "    freq = math.log(10000) / (half_dim - 1)\n",
    "    freq = torch.exp(torch.arange(half_dim) * -freq)\n",
    "    \n",
    "    # Position indices\n",
    "    pos = torch.arange(grid_size).float()\n",
    "    \n",
    "    # Compute PE: pos √ó freq gives the angle\n",
    "    angles = pos[:, None] * freq[None, :]  # (grid_size, half_dim)\n",
    "    sin_emb = torch.sin(angles)\n",
    "    cos_emb = torch.cos(angles)\n",
    "    pos_1d = torch.cat([sin_emb, cos_emb], dim=-1)  # (grid_size, embed_dim/2)\n",
    "    \n",
    "    # Create 2D embeddings\n",
    "    row_emb = pos_1d.unsqueeze(1).expand(-1, grid_size, -1)\n",
    "    col_emb = pos_1d.unsqueeze(0).expand(grid_size, -1, -1)\n",
    "    pos_2d = torch.cat([row_emb, col_emb], dim=-1)  # (7, 7, 256)\n",
    "    pos_flat = pos_2d.view(-1, embed_dim)  # (49, 256)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # 1D embeddings\n",
    "    im = axes[0, 0].imshow(pos_1d.numpy(), aspect='auto', cmap='RdBu')\n",
    "    axes[0, 0].set_xlabel('Dimension')\n",
    "    axes[0, 0].set_ylabel('Position (0-6)')\n",
    "    axes[0, 0].set_title('1D Sinusoidal Encoding\\n$PE(pos) = [\\\\sin(pos/\\\\omega_i), \\\\cos(pos/\\\\omega_i)]$')\n",
    "    plt.colorbar(im, ax=axes[0, 0])\n",
    "    \n",
    "    # Full 2D embedding matrix\n",
    "    im = axes[0, 1].imshow(pos_flat.numpy(), aspect='auto', cmap='RdBu')\n",
    "    axes[0, 1].set_xlabel('Embedding dimension')\n",
    "    axes[0, 1].set_ylabel('Patch index (0-48)')\n",
    "    axes[0, 1].set_title(f'2D Positional Embeddings\\n$PE_{{2D}}(r,c) \\\\in \\\\mathbb{{R}}^{{{embed_dim}}}$')\n",
    "    plt.colorbar(im, ax=axes[0, 1])\n",
    "    \n",
    "    # Dot-product similarity\n",
    "    similarity = torch.mm(pos_flat, pos_flat.T)\n",
    "    similarity = similarity / similarity.max()\n",
    "    im = axes[1, 0].imshow(similarity.numpy(), cmap='viridis')\n",
    "    axes[1, 0].set_xlabel('Patch index')\n",
    "    axes[1, 0].set_ylabel('Patch index')\n",
    "    axes[1, 0].set_title('Position Similarity: $PE_i \\\\cdot PE_j$\\n(brighter = more similar)')\n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "    \n",
    "    # Spatial similarity from center\n",
    "    center = 24  # Center of 7√ó7 grid\n",
    "    center_sim = similarity[center].view(grid_size, grid_size)\n",
    "    im = axes[1, 1].imshow(center_sim.numpy(), cmap='viridis')\n",
    "    axes[1, 1].plot(3, 3, 'r*', markersize=20, label='Query patch')\n",
    "    axes[1, 1].set_title(f'Similarity to center patch\\n(Manhattan distance pattern)')\n",
    "    axes[1, 1].legend()\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Positional Embedding Properties:\")\n",
    "    print(f\"   ‚Ä¢ Each position gets a unique {embed_dim}D vector\")\n",
    "    print(f\"   ‚Ä¢ Nearby patches have similar embeddings (high dot product)\")\n",
    "    print(f\"   ‚Ä¢ Distant patches have dissimilar embeddings (low dot product)\")\n",
    "    print(f\"   ‚Ä¢ The model can learn to interpret these patterns\")\n",
    "\n",
    "visualize_positional_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-theory",
   "metadata": {},
   "source": [
    "## 3. Self-Attention: The Heart of Transformers\n",
    "\n",
    "Self-attention is what gives transformers their power. Unlike CNNs where each location only sees its local neighborhood, self-attention lets **every token attend to every other token** in a single operation.\n",
    "\n",
    "### The Attention Mechanism\n",
    "\n",
    "Given input tokens $X \\in \\mathbb{R}^{N \\times D}$ (N tokens, D dimensions each):\n",
    "\n",
    "1. **Compute Query, Key, Value** via learned projections:\n",
    "   $$Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V$$\n",
    "   where $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$\n",
    "\n",
    "2. **Compute attention scores** (how much should token $i$ attend to token $j$?):\n",
    "   $$A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{D}}\\right) \\in \\mathbb{R}^{N \\times N}$$\n",
    "\n",
    "3. **Aggregate values** based on attention weights:\n",
    "   $$\\text{Output} = AV \\in \\mathbb{R}^{N \\times D}$$\n",
    "\n",
    "### Understanding Each Component\n",
    "\n",
    "| Component | Intuition | Shape |\n",
    "|-----------|-----------|-------|\n",
    "| **Query (Q)** | \"What am I looking for?\" | $N \\times D$ |\n",
    "| **Key (K)** | \"What do I contain?\" | $N \\times D$ |\n",
    "| **Value (V)** | \"What information do I provide?\" | $N \\times D$ |\n",
    "| **Attention (A)** | \"How much should I attend to each token?\" | $N \\times N$ |\n",
    "\n",
    "### The Scaling Factor $\\sqrt{D}$\n",
    "\n",
    "Why divide by $\\sqrt{D}$? Without it:\n",
    "- Dot products $q_i \\cdot k_j$ grow with dimension $D$\n",
    "- Large values ‚Üí softmax becomes nearly one-hot\n",
    "- Gradients vanish, training fails\n",
    "\n",
    "Dividing by $\\sqrt{D}$ keeps the variance of dot products ‚âà 1.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one attention operation, we run $H$ parallel \"heads\":\n",
    "\n",
    "$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_H) W_O$$\n",
    "\n",
    "where each head has dimension $D/H$. This lets the model attend to different aspects simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_mechanism():\n",
    "    \"\"\"\n",
    "    Visualize the self-attention computation step by step.\n",
    "    \"\"\"\n",
    "    N = 7  # 7 tokens (simplified for visualization)\n",
    "    D = 4  # 4 dimensions\n",
    "    \n",
    "    # Create sample input\n",
    "    np.random.seed(42)\n",
    "    X = torch.randn(N, D)\n",
    "    \n",
    "    # Learned projections (random for illustration)\n",
    "    W_Q = torch.randn(D, D) * 0.5\n",
    "    W_K = torch.randn(D, D) * 0.5\n",
    "    W_V = torch.randn(D, D) * 0.5\n",
    "    \n",
    "    # Compute Q, K, V\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = Q @ K.T / np.sqrt(D)\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = attention @ V\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Row 1: The computation\n",
    "    vmin, vmax = -2, 2\n",
    "    \n",
    "    axes[0, 0].imshow(X.numpy(), cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axes[0, 0].set_title('Input $X$\\n$(N \\\\times D)$', fontsize=11)\n",
    "    axes[0, 0].set_xlabel('Dimensions')\n",
    "    axes[0, 0].set_ylabel('Tokens')\n",
    "    \n",
    "    axes[0, 1].imshow(Q.numpy(), cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axes[0, 1].set_title('Query $Q = XW_Q$\\n\"What am I looking for?\"', fontsize=11)\n",
    "    axes[0, 1].set_xlabel('Dimensions')\n",
    "    \n",
    "    axes[0, 2].imshow(K.numpy(), cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axes[0, 2].set_title('Key $K = XW_K$\\n\"What do I contain?\"', fontsize=11)\n",
    "    axes[0, 2].set_xlabel('Dimensions')\n",
    "    \n",
    "    axes[0, 3].imshow(V.numpy(), cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axes[0, 3].set_title('Value $V = XW_V$\\n\"What info do I provide?\"', fontsize=11)\n",
    "    axes[0, 3].set_xlabel('Dimensions')\n",
    "    \n",
    "    # Row 2: Attention computation\n",
    "    axes[1, 0].imshow(scores.numpy(), cmap='RdBu')\n",
    "    axes[1, 0].set_title('Scores: $QK^T / \\\\sqrt{D}$', fontsize=11)\n",
    "    axes[1, 0].set_xlabel('Key token')\n",
    "    axes[1, 0].set_ylabel('Query token')\n",
    "    \n",
    "    im = axes[1, 1].imshow(attention.numpy(), cmap='Reds')\n",
    "    axes[1, 1].set_title('Attention: $\\\\text{softmax}(\\\\cdot)$\\n(rows sum to 1)', fontsize=11)\n",
    "    axes[1, 1].set_xlabel('Key token')\n",
    "    axes[1, 1].set_ylabel('Query token')\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "    \n",
    "    axes[1, 2].imshow(output.numpy(), cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axes[1, 2].set_title('Output: $\\\\text{Attention} \\\\times V$', fontsize=11)\n",
    "    axes[1, 2].set_xlabel('Dimensions')\n",
    "    axes[1, 2].set_ylabel('Tokens')\n",
    "    \n",
    "    # Show attention pattern for one query\n",
    "    query_idx = 3\n",
    "    axes[1, 3].bar(range(N), attention[query_idx].numpy())\n",
    "    axes[1, 3].set_title(f'Token {query_idx} attends to:', fontsize=11)\n",
    "    axes[1, 3].set_xlabel('Token index')\n",
    "    axes[1, 3].set_ylabel('Attention weight')\n",
    "    axes[1, 3].set_ylim(0, 1)\n",
    "    \n",
    "    plt.suptitle('Self-Attention: $\\\\text{Attention}(Q,K,V) = \\\\text{softmax}(QK^T/\\\\sqrt{D}) \\\\cdot V$', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Attention Properties:\")\n",
    "    print(f\"   ‚Ä¢ Input: {N} tokens √ó {D} dimensions\")\n",
    "    print(f\"   ‚Ä¢ Attention matrix: {N}√ó{N} = {N**2} pairwise interactions\")\n",
    "    print(f\"   ‚Ä¢ Each row sums to 1 (probability distribution)\")\n",
    "    print(f\"   ‚Ä¢ Computation: O(N¬≤D) - quadratic in sequence length\")\n",
    "\n",
    "visualize_attention_mechanism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_attention_patterns():\n",
    "    \"\"\"\n",
    "    Show what attention patterns look like for image patches.\n",
    "    \"\"\"\n",
    "    grid_size = 7\n",
    "    N = grid_size ** 2  # 49 patches\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    query_pos = (3, 3)  # Center patch\n",
    "    query_idx = query_pos[0] * grid_size + query_pos[1]\n",
    "    \n",
    "    patterns = [\n",
    "        ('Uniform', np.ones((grid_size, grid_size)) / N),\n",
    "        ('Local (Gaussian)', None),  # Will compute\n",
    "        ('Horizontal', None),\n",
    "        ('Vertical', None),\n",
    "        ('Diagonal', None),\n",
    "        ('Sparse', None),\n",
    "        ('Learned (example)', None),\n",
    "        ('Cross', None),\n",
    "    ]\n",
    "    \n",
    "    # Compute patterns\n",
    "    # Local Gaussian\n",
    "    local = np.zeros((grid_size, grid_size))\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            dist = np.sqrt((i - query_pos[0])**2 + (j - query_pos[1])**2)\n",
    "            local[i, j] = np.exp(-dist**2 / 2)\n",
    "    patterns[1] = ('Local (Gaussian)', local / local.sum())\n",
    "    \n",
    "    # Horizontal\n",
    "    horiz = np.zeros((grid_size, grid_size))\n",
    "    horiz[query_pos[0], :] = 1\n",
    "    patterns[2] = ('Horizontal', horiz / horiz.sum())\n",
    "    \n",
    "    # Vertical\n",
    "    vert = np.zeros((grid_size, grid_size))\n",
    "    vert[:, query_pos[1]] = 1\n",
    "    patterns[3] = ('Vertical', vert / vert.sum())\n",
    "    \n",
    "    # Diagonal\n",
    "    diag = np.zeros((grid_size, grid_size))\n",
    "    for i in range(grid_size):\n",
    "        diag[i, i] = 1\n",
    "        if grid_size - 1 - i >= 0:\n",
    "            diag[i, grid_size - 1 - i] = 0.5\n",
    "    patterns[4] = ('Diagonal', diag / diag.sum())\n",
    "    \n",
    "    # Sparse (corners + center)\n",
    "    sparse = np.zeros((grid_size, grid_size))\n",
    "    sparse[0, 0] = sparse[0, -1] = sparse[-1, 0] = sparse[-1, -1] = 1\n",
    "    sparse[3, 3] = 2\n",
    "    patterns[5] = ('Corners + Center', sparse / sparse.sum())\n",
    "    \n",
    "    # Learned-like (mixture)\n",
    "    learned = local + 0.5 * horiz + 0.3 * np.random.rand(grid_size, grid_size)\n",
    "    patterns[6] = ('Learned (mixed)', learned / learned.sum())\n",
    "    \n",
    "    # Cross\n",
    "    cross = horiz + vert\n",
    "    patterns[7] = ('Cross', cross / cross.sum())\n",
    "    \n",
    "    for idx, (name, pattern) in enumerate(patterns):\n",
    "        ax = axes[idx // 4, idx % 4]\n",
    "        im = ax.imshow(pattern, cmap='Reds', vmin=0)\n",
    "        ax.plot(query_pos[1], query_pos[0], 'b*', markersize=15)\n",
    "        ax.set_title(name, fontsize=11)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Possible Attention Patterns (‚òÖ = query patch, red = attention weights)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Attention patterns the model might learn:\")\n",
    "    print(\"   ‚Ä¢ Early layers: Local patterns (like CNN kernels)\")\n",
    "    print(\"   ‚Ä¢ Middle layers: Structural patterns (horizontal, vertical strokes)\")\n",
    "    print(\"   ‚Ä¢ Late layers: Semantic patterns (attend to digit-specific regions)\")\n",
    "    print(\"   ‚Ä¢ The model learns which patterns are useful for the task!\")\n",
    "\n",
    "visualize_image_attention_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaln-theory",
   "metadata": {},
   "source": [
    "## 4. Adaptive Layer Normalization (adaLN)\n",
    "\n",
    "How do we tell the model what timestep we're at? In Phase 1, we simply added the timestep embedding to feature maps. DiT uses a more powerful approach: **Adaptive Layer Normalization**.\n",
    "\n",
    "### Standard Layer Normalization\n",
    "\n",
    "Layer Norm normalizes activations and applies learned affine transform:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "where:\n",
    "- $\\mu, \\sigma$ = mean and std of $x$ (computed per-sample)\n",
    "- $\\gamma, \\beta$ = **learned** scale and shift parameters\n",
    "\n",
    "### Adaptive Layer Norm (adaLN)\n",
    "\n",
    "Instead of **learned** $\\gamma, \\beta$, we **predict** them from the timestep:\n",
    "\n",
    "$$[\\gamma, \\beta] = \\text{MLP}(\\text{time\\_embed}(t))$$\n",
    "$$\\text{adaLN}(x, t) = \\gamma(t) \\odot \\frac{x - \\mu}{\\sigma} + \\beta(t)$$\n",
    "\n",
    "### Why This Is Powerful\n",
    "\n",
    "| Approach | Conditioning Strength |\n",
    "|----------|----------------------|\n",
    "| **Additive** $(x + t_{emb})$ | Weak: just shifts activations |\n",
    "| **Concatenation** $[x, t_{emb}]$ | Medium: separate channels |\n",
    "| **adaLN** | Strong: scales AND shifts every activation |\n",
    "\n",
    "adaLN modulates the **entire distribution** of activations, not just the mean. At each timestep, the model can:\n",
    "- Amplify certain features ($\\gamma > 1$)\n",
    "- Suppress others ($\\gamma \\approx 0$)\n",
    "- Shift the operating point ($\\beta$)\n",
    "\n",
    "### Timestep-Dependent Behavior\n",
    "\n",
    "- **$t \\approx 1$ (mostly noise)**: Model needs to find large-scale structure\n",
    "- **$t \\approx 0$ (mostly data)**: Model needs to refine fine details\n",
    "\n",
    "adaLN lets the model behave **completely differently** at different timesteps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-adaln",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.dit import TimestepEmbedding, AdaLN\n",
    "\n",
    "def visualize_adaln_modulation():\n",
    "    \"\"\"\n",
    "    Show how adaLN parameters vary with timestep.\n",
    "    \"\"\"\n",
    "    embed_dim = 256\n",
    "    cond_dim = embed_dim * 4\n",
    "    \n",
    "    time_embed = TimestepEmbedding(embed_dim, cond_dim)\n",
    "    adaln = AdaLN(embed_dim, cond_dim)\n",
    "    \n",
    "    timesteps = torch.linspace(0, 1, 100)\n",
    "    \n",
    "    scales = []\n",
    "    shifts = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in timesteps:\n",
    "            cond = time_embed(t.unsqueeze(0))\n",
    "            params = adaln.proj(cond)\n",
    "            scale, shift = params.chunk(2, dim=-1)\n",
    "            scales.append(scale.squeeze().numpy())\n",
    "            shifts.append(shift.squeeze().numpy())\n",
    "    \n",
    "    scales = np.array(scales)  # (100, 256)\n",
    "    shifts = np.array(shifts)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Scale heatmap\n",
    "    im = axes[0, 0].imshow(scales.T, aspect='auto', cmap='RdBu', \n",
    "                           extent=[0, 1, embed_dim, 0], vmin=-1, vmax=1)\n",
    "    axes[0, 0].set_xlabel('Timestep $t$')\n",
    "    axes[0, 0].set_ylabel('Dimension')\n",
    "    axes[0, 0].set_title('Scale $\\\\gamma(t)$: How much to amplify each dimension')\n",
    "    plt.colorbar(im, ax=axes[0, 0])\n",
    "    \n",
    "    # Shift heatmap\n",
    "    im = axes[0, 1].imshow(shifts.T, aspect='auto', cmap='RdBu',\n",
    "                           extent=[0, 1, embed_dim, 0], vmin=-1, vmax=1)\n",
    "    axes[0, 1].set_xlabel('Timestep $t$')\n",
    "    axes[0, 1].set_ylabel('Dimension')\n",
    "    axes[0, 1].set_title('Shift $\\\\beta(t)$: How much to shift each dimension')\n",
    "    plt.colorbar(im, ax=axes[0, 1])\n",
    "    \n",
    "    # Selected dimensions\n",
    "    for d in [0, 64, 128, 192]:\n",
    "        axes[1, 0].plot(timesteps.numpy(), scales[:, d], label=f'dim {d}')\n",
    "    axes[1, 0].set_xlabel('Timestep $t$')\n",
    "    axes[1, 0].set_ylabel('Scale $\\\\gamma$')\n",
    "    axes[1, 0].set_title('Scale values for selected dimensions')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mean ¬± std\n",
    "    mean_scale = scales.mean(axis=1)\n",
    "    std_scale = scales.std(axis=1)\n",
    "    mean_shift = shifts.mean(axis=1)\n",
    "    std_shift = shifts.std(axis=1)\n",
    "    \n",
    "    axes[1, 1].plot(timesteps.numpy(), mean_scale, 'b-', label='Mean scale', linewidth=2)\n",
    "    axes[1, 1].fill_between(timesteps.numpy(), mean_scale - std_scale, mean_scale + std_scale, alpha=0.3)\n",
    "    axes[1, 1].plot(timesteps.numpy(), mean_shift, 'r-', label='Mean shift', linewidth=2)\n",
    "    axes[1, 1].fill_between(timesteps.numpy(), mean_shift - std_shift, mean_shift + std_shift, alpha=0.3, color='red')\n",
    "    axes[1, 1].set_xlabel('Timestep $t$')\n",
    "    axes[1, 1].set_ylabel('Parameter value')\n",
    "    axes[1, 1].set_title('Mean adaLN parameters (¬±1 std)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Adaptive Layer Norm: $\\\\text{adaLN}(x, t) = \\\\gamma(t) \\\\odot \\\\text{Norm}(x) + \\\\beta(t)$', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä adaLN Insights:\")\n",
    "    print(\"   ‚Ä¢ Each timestep produces different Œ≥ and Œ≤ values\")\n",
    "    print(\"   ‚Ä¢ The model can amplify/suppress different features at different t\")\n",
    "    print(\"   ‚Ä¢ This is much more expressive than just adding timestep embeddings\")\n",
    "    print(f\"   ‚Ä¢ Formula: output = Œ≥(t) √ó LayerNorm(x) + Œ≤(t)\")\n",
    "\n",
    "visualize_adaln_modulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-theory",
   "metadata": {},
   "source": [
    "## 5. The Complete DiT Architecture\n",
    "\n",
    "Now let's put all the pieces together. The DiT architecture processes images through:\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "1. **Patchify**: $x \\in \\mathbb{R}^{H \\times W \\times C} \\rightarrow p \\in \\mathbb{R}^{N \\times (P^2 C)}$\n",
    "2. **Embed**: $z = pE + \\text{PE}_{2D} \\in \\mathbb{R}^{N \\times D}$\n",
    "3. **Condition**: $c = \\text{MLP}(\\text{sinusoidal}(t)) \\in \\mathbb{R}^{D_c}$\n",
    "4. **Transform**: For $l = 1, \\ldots, L$:\n",
    "   - $z' = z + \\text{Attention}(\\text{adaLN}(z, c))$\n",
    "   - $z = z' + \\text{MLP}(\\text{adaLN}(z', c))$\n",
    "5. **Unpatchify**: $z \\in \\mathbb{R}^{N \\times D} \\rightarrow v \\in \\mathbb{R}^{H \\times W \\times C}$\n",
    "\n",
    "### DiT Block Structure\n",
    "\n",
    "```\n",
    "Input z\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ                                   ‚îÇ (residual)\n",
    "   ‚ñº                                   ‚îÇ\n",
    "adaLN(z, c) ‚îÄ‚îÄ‚ñ∫ Self-Attention ‚îÄ‚îÄ‚ñ∫ + ‚îÄ‚îÄ‚î§\n",
    "                                       ‚îÇ\n",
    "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "   ‚îÇ                                   ‚îÇ (residual)\n",
    "   ‚ñº                                   ‚îÇ\n",
    "adaLN(z, c) ‚îÄ‚îÄ‚ñ∫ MLP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ + ‚îÄ‚îÄ‚îò\n",
    "   ‚îÇ\n",
    "   ‚ñº\n",
    "Output z\n",
    "```\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "| Component | Complexity | For MNIST (N=49, D=256) |\n",
    "|-----------|------------|-------------------------|\n",
    "| Self-Attention | $O(N^2 D)$ | 49¬≤ √ó 256 ‚âà 600K |\n",
    "| MLP | $O(N D^2)$ | 49 √ó 256¬≤ ‚âà 3.2M |\n",
    "| Per Block | $O(N^2 D + N D^2)$ | ‚âà 3.8M ops |\n",
    "| Full Model (6 blocks) | | ‚âà 23M ops |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.dit import DiT\n",
    "\n",
    "# Create DiT model\n",
    "model = DiT(\n",
    "    img_size=28,       # MNIST\n",
    "    patch_size=4,      # 7√ó7 = 49 patches\n",
    "    in_channels=1,     # Grayscale\n",
    "    embed_dim=256,     # Embedding dimension D\n",
    "    depth=6,           # Number of transformer blocks L\n",
    "    num_heads=8,       # Attention heads H (256/8 = 32 dim per head)\n",
    "    mlp_ratio=4.0,     # MLP hidden = 256 √ó 4 = 1024\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"DiT Parameters: {num_params:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"   Patches: 7√ó7 = 49 (sequence length N)\")\n",
    "print(f\"   Embedding: D = 256\")\n",
    "print(f\"   Heads: H = 8 (head dim = 256/8 = 32)\")\n",
    "print(f\"   MLP hidden: 256 √ó 4 = 1024\")\n",
    "print(f\"   Blocks: L = 6\")\n",
    "\n",
    "# Test forward pass\n",
    "test_x = torch.randn(4, 1, 28, 28, device=device)\n",
    "test_t = torch.rand(4, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_out = model(test_x, test_t)\n",
    "\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"   Input:  {test_x.shape}\")\n",
    "print(f\"   Output: {test_out.shape}\")\n",
    "print(f\"   ‚úì Output shape matches input (predicts velocity field)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trace-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace through the model step by step\n",
    "print(\"=\" * 65)\n",
    "print(\"TRACING DiT FORWARD PASS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28, device=device)\n",
    "t = torch.tensor([0.5], device=device)\n",
    "\n",
    "print(f\"\\n1. INPUT\")\n",
    "print(f\"   x ‚àà ‚Ñù^{tuple(x.shape)} (image)\")\n",
    "print(f\"   t = {t.item():.1f} (timestep)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Patchify\n",
    "    patches = model.patch_embed(x)\n",
    "    print(f\"\\n2. PATCHIFY\")\n",
    "    print(f\"   Embed patches: {x.shape} ‚Üí {patches.shape}\")\n",
    "    print(f\"   (28√ó28 image ‚Üí 49 patches √ó 256 dim)\")\n",
    "    \n",
    "    # Position embed\n",
    "    patches_pos = model.pos_embed(patches)\n",
    "    print(f\"\\n3. POSITIONAL EMBEDDING\")\n",
    "    print(f\"   Add PE: {patches_pos.shape}\")\n",
    "    print(f\"   (Each patch knows its 2D position)\")\n",
    "    \n",
    "    # Time embed\n",
    "    cond = model.time_embed(t)\n",
    "    print(f\"\\n4. TIMESTEP CONDITIONING\")\n",
    "    print(f\"   t={t.item():.1f} ‚Üí cond ‚àà ‚Ñù^{tuple(cond.shape)}\")\n",
    "    print(f\"   (Sinusoidal encoding ‚Üí MLP ‚Üí conditioning vector)\")\n",
    "    \n",
    "    # Transformer blocks\n",
    "    print(f\"\\n5. TRANSFORMER BLOCKS (√ó{len(model.blocks)})\")\n",
    "    print(f\"   Each block: adaLN ‚Üí Attention ‚Üí adaLN ‚Üí MLP\")\n",
    "    print(f\"   Residual connections preserve information\")\n",
    "    \n",
    "    # Output\n",
    "    output = model(x, t)\n",
    "    print(f\"\\n6. OUTPUT\")\n",
    "    print(f\"   Final norm ‚Üí Linear ‚Üí Unpatchify\")\n",
    "    print(f\"   {patches.shape} ‚Üí {output.shape}\")\n",
    "    print(f\"   (Predicted velocity field)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-theory",
   "metadata": {},
   "source": [
    "## 6. Training: Same Objective, Different Architecture\n",
    "\n",
    "The beauty of flow matching is that the training objective is **architecture-agnostic**:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, x_1, t}\\left[\\|v_\\theta(x_t, t) - (x_1 - x_0)\\|^2\\right]$$\n",
    "\n",
    "We can swap the U-Net for DiT without changing anything else:\n",
    "- Same forward process: $x_t = (1-t)x_0 + tx_1$\n",
    "- Same velocity target: $v = x_1 - x_0$\n",
    "- Same loss: MSE between predicted and true velocity\n",
    "- Same sampling: Euler integration of the ODE\n",
    "\n",
    "### Training Considerations for Transformers\n",
    "\n",
    "| Aspect | CNN | DiT |\n",
    "|--------|-----|-----|\n",
    "| Parameters | ~1.8M | ~12.4M |\n",
    "| Memory | Lower | Higher (attention matrices) |\n",
    "| Speed | Faster per step | Slower per step |\n",
    "| Convergence | Quick | Needs more epochs |\n",
    "| Scaling | Diminishing returns | Predictable improvement |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.train import Trainer\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Training DiT with flow matching objective...\")\n",
    "print(f\"Loss: L = ||v_Œ∏(x_t, t) - (x_1 - x_0)||¬≤\")\n",
    "print()\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "losses = trainer.train(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, marker='o', markersize=4)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('DiT Training: $\\\\mathcal{L} = \\\\|v_\\\\theta(x_t, t) - v\\\\|^2$', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation-theory",
   "metadata": {},
   "source": [
    "## 7. Generation: Sampling from the Learned Model\n",
    "\n",
    "Generation uses the same ODE as Phase 1:\n",
    "\n",
    "$$\\frac{dx}{dt} = v_\\theta(x, t)$$\n",
    "\n",
    "Starting from $x_1 \\sim \\mathcal{N}(0, I)$ at $t=1$, integrate backward to $t=0$:\n",
    "\n",
    "$$x_{t-\\Delta t} = x_t - \\Delta t \\cdot v_\\theta(x_t, t)$$\n",
    "\n",
    "The DiT's global attention should produce more coherent samples than the CNN's local processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opticus.sampling import sample\n",
    "\n",
    "def show_images(images, nrow=8, title=\"\"):\n",
    "    images = (images + 1) / 2\n",
    "    images = images.clamp(0, 1)\n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow, padding=2)\n",
    "    plt.figure(figsize=(12, 12 * grid.shape[1] / grid.shape[2]))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated, trajectory = sample(\n",
    "        model=model,\n",
    "        num_samples=64,\n",
    "        image_shape=(1, 28, 28),\n",
    "        num_steps=50,\n",
    "        device=device,\n",
    "        return_trajectory=True,\n",
    "    )\n",
    "\n",
    "show_images(generated, nrow=8, title=\"DiT Generated Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generation process\n",
    "num_to_show = 4\n",
    "steps_to_show = [0, 5, 10, 20, 30, 40, 50]\n",
    "\n",
    "fig, axes = plt.subplots(num_to_show, len(steps_to_show), figsize=(14, 8))\n",
    "\n",
    "for row in range(num_to_show):\n",
    "    for col, step_idx in enumerate(steps_to_show):\n",
    "        img = (trajectory[step_idx][row, 0] + 1) / 2\n",
    "        axes[row, col].imshow(img.cpu().numpy(), cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "        if row == 0:\n",
    "            t_val = 1.0 - step_idx / 50\n",
    "            axes[row, col].set_title(f'$t={t_val:.2f}$')\n",
    "\n",
    "plt.suptitle('DiT Generation: Solving $dx/dt = v_\\\\theta(x,t)$ from $t=1$ to $t=0$', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-intro",
   "metadata": {},
   "source": [
    "## 8. CNN vs DiT Comparison\n",
    "\n",
    "Let's directly compare the two architectures:\n",
    "\n",
    "| Architecture | Parameters | Receptive Field | Conditioning |\n",
    "|--------------|------------|-----------------|---------------|\n",
    "| **U-Net (CNN)** | ~1.8M | Local ‚Üí Global (via depth) | Additive |\n",
    "| **DiT** | ~12.4M | Global (from layer 1) | adaLN (multiplicative) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from opticus.models import SimpleUNet\n",
    "\n",
    "# Load or train CNN\n",
    "cnn_model = SimpleUNet(in_channels=1, model_channels=64, time_emb_dim=128).to(device)\n",
    "\n",
    "if os.path.exists(\"phase1_model.pt\"):\n",
    "    print(\"Loading CNN from phase1_model.pt...\")\n",
    "    checkpoint = torch.load(\"phase1_model.pt\", map_location=device)\n",
    "    cnn_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    print(\"Training CNN for comparison...\")\n",
    "    cnn_trainer = Trainer(model=cnn_model, dataloader=train_loader, lr=1e-4, device=device)\n",
    "    cnn_trainer.train(num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Generate from both\n",
    "model.eval()\n",
    "cnn_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dit_samples = sample(model, 16, (1, 28, 28), 50, device)\n",
    "    cnn_samples = sample(cnn_model, 16, (1, 28, 28), 50, device)\n",
    "\n",
    "real_samples = torch.stack([train_dataset[i][0] for i in range(16)])\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "for ax, samples, title in [\n",
    "    (axes[0], cnn_samples, 'CNN (U-Net)\\n~1.8M params'),\n",
    "    (axes[1], dit_samples, 'DiT\\n~12.4M params'),\n",
    "    (axes[2], real_samples, 'Real MNIST'),\n",
    "]:\n",
    "    grid = torchvision.utils.make_grid((samples + 1) / 2, nrow=4, padding=2)\n",
    "    ax.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Architecture Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary: Key Concepts\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A Diffusion Transformer that processes images through:\n",
    "\n",
    "1. **Patchification**: $x \\in \\mathbb{R}^{28 \\times 28} \\rightarrow z \\in \\mathbb{R}^{49 \\times 256}$\n",
    "2. **Positional Embeddings**: 2D sinusoidal encoding for spatial structure\n",
    "3. **Self-Attention**: $\\text{Attention}(Q,K,V) = \\text{softmax}(QK^T/\\sqrt{d})V$\n",
    "4. **adaLN**: $\\text{adaLN}(x,t) = \\gamma(t) \\cdot \\text{Norm}(x) + \\beta(t)$\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "| Concept | Formula | Purpose |\n",
    "|---------|---------|--------|\n",
    "| Patchify | $z = xE + PE$ | Image ‚Üí sequence |\n",
    "| Attention | $\\text{softmax}(QK^T/\\sqrt{d})V$ | Global interactions |\n",
    "| adaLN | $\\gamma(t) \\odot \\text{Norm}(x) + \\beta(t)$ | Timestep conditioning |\n",
    "| Training | $\\|v_\\theta(x_t,t) - v\\|^2$ | Learn velocity field |\n",
    "\n",
    "### Why DiT Matters\n",
    "\n",
    "- **Global from layer 1**: Every patch sees every other patch\n",
    "- **Scaling laws**: Predictable improvement with more compute\n",
    "- **Strong conditioning**: adaLN modulates all activations\n",
    "- **Architecture simplicity**: Stack identical blocks\n",
    "\n",
    "## Next: Phase 3\n",
    "\n",
    "Add **class conditioning** to control generation:\n",
    "- Class embeddings combined with timestep\n",
    "- Classifier-Free Guidance (CFG)\n",
    "- \"Generate a 7\" ‚Üí produces a 7!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# trainer.save_checkpoint(\"phase2_dit.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
